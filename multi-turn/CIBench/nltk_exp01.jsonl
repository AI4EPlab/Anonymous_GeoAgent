{"prompt": "Text Content: \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
