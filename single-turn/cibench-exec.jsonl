{"prompt": "File Path: \"data/seaborn_dataset04.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a scatter plot for 'Glucose' against 'BMI', using color = \"blue\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import seaborn as sns\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", color=\"blue\").set(title=\"Glucose vs BMI\", xlabel=\"Glucose\", ylabel=\"BMI\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a joint plot for 'BloodPressure' and 'Age', using height = 6, color = \"green\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", color=\"blue\").set(title=\"Glucose vs BMI\", xlabel=\"Glucose\", ylabel=\"BMI\")\n\n", "code": "sns.jointplot(data=df, x=\"BloodPressure\", y=\"Age\", color=\"green\", height=6).set_axis_labels(\"Blood Pressure\", \"Age\").fig.suptitle(\"Blood Pressure vs Age\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a pair plot for all the numerical columns, using color = \"pastel\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", color=\"blue\").set(title=\"Glucose vs BMI\", xlabel=\"Glucose\", ylabel=\"BMI\")\n\nsns.jointplot(data=df, x=\"BloodPressure\", y=\"Age\", color=\"green\", height=6).set_axis_labels(\"Blood Pressure\", \"Age\").fig.suptitle(\"Blood Pressure vs Age\")\n\n", "code": "# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Diabetes Dataset Pairplot\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a violin plot for 'DiabetesPedigreeFunction' based on different outcome, using color = \"orange\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", color=\"blue\").set(title=\"Glucose vs BMI\", xlabel=\"Glucose\", ylabel=\"BMI\")\n\nsns.jointplot(data=df, x=\"BloodPressure\", y=\"Age\", color=\"green\", height=6).set_axis_labels(\"Blood Pressure\", \"Age\").fig.suptitle(\"Blood Pressure vs Age\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Diabetes Dataset Pairplot\")\n\n", "code": "sns.violinplot(data=df, x=\"Outcome\", y=\"DiabetesPedigreeFunction\", color=\"orange\").set(title=\"Violin Plot of 'DiabetesPedigreeFunction'\", xlabel=\"Outcome\", ylabel=\"DiabetesPedigreeFunction\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nCreate a scatterplot of Glucose and Insulin that colors points based on the 'Outcome' column, using color = \"bright\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", color=\"blue\").set(title=\"Glucose vs BMI\", xlabel=\"Glucose\", ylabel=\"BMI\")\n\nsns.jointplot(data=df, x=\"BloodPressure\", y=\"Age\", color=\"green\", height=6).set_axis_labels(\"Blood Pressure\", \"Age\").fig.suptitle(\"Blood Pressure vs Age\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Diabetes Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"Outcome\", y=\"DiabetesPedigreeFunction\", color=\"orange\").set(title=\"Violin Plot of 'DiabetesPedigreeFunction'\", xlabel=\"Outcome\", ylabel=\"DiabetesPedigreeFunction\")\n\n\n", "code": "sns.scatterplot(data=df, x=\"Glucose\", y=\"Insulin\", hue=\"Outcome\", palette=\"bright\").set(title=\"Outcome-based Scatterplot\", xlabel=\"Glucose\", ylabel=\"Insulin\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nCreate a scatterplot with a regression line to visualize the relationship between 'SkinThickness' and 'BMI', using color = \"plum\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"Glucose\", y=\"BMI\", color=\"blue\").set(title=\"Glucose vs BMI\", xlabel=\"Glucose\", ylabel=\"BMI\")\n\nsns.jointplot(data=df, x=\"BloodPressure\", y=\"Age\", color=\"green\", height=6).set_axis_labels(\"Blood Pressure\", \"Age\").fig.suptitle(\"Blood Pressure vs Age\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Diabetes Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"Outcome\", y=\"DiabetesPedigreeFunction\", color=\"orange\").set(title=\"Violin Plot of 'DiabetesPedigreeFunction'\", xlabel=\"Outcome\", ylabel=\"DiabetesPedigreeFunction\")\n\n\nsns.scatterplot(data=df, x=\"Glucose\", y=\"Insulin\", hue=\"Outcome\", palette=\"bright\").set(title=\"Outcome-based Scatterplot\", xlabel=\"Glucose\", ylabel=\"Insulin\")\n\n", "code": "sns.regplot(data=df, x=\"SkinThickness\", y=\"BMI\", color=\"plum\").set(title=\"Skin Thickness vs BMI\", xlabel=\"Skin Thickness\", ylabel=\"BMI\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "File Path: \"data/seaborn_dataset05.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/seaborn_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a scatter plot for 'radius_mean' against 'texture_mean', using color = \"blue\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import seaborn as sns\nsns.scatterplot(data=df, x=\"radius_mean\", y=\"texture_mean\", color=\"blue\").set(title=\"Radius Mean vs Texture Mean\", xlabel=\"Radius Mean\", ylabel=\"Texture Mean\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a joint plot for 'area_mean' and 'smoothness_mean', using height = 6, color = \"green\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"radius_mean\", y=\"texture_mean\", color=\"blue\").set(title=\"Radius Mean vs Texture Mean\", xlabel=\"Radius Mean\", ylabel=\"Texture Mean\")\n\n", "code": "sns.jointplot(data=df, x=\"area_mean\", y=\"smoothness_mean\", color=\"green\", height=6).set_axis_labels(\"Area Mean\", \"Smoothness Mean\").fig.suptitle(\"Area Mean vs Smoothness Mean\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a pair plot for the first five numerical columns, using color = \"pastel\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"radius_mean\", y=\"texture_mean\", color=\"blue\").set(title=\"Radius Mean vs Texture Mean\", xlabel=\"Radius Mean\", ylabel=\"Texture Mean\")\n\nsns.jointplot(data=df, x=\"area_mean\", y=\"smoothness_mean\", color=\"green\", height=6).set_axis_labels(\"Area Mean\", \"Smoothness Mean\").fig.suptitle(\"Area Mean vs Smoothness Mean\")\n\n", "code": "# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ndf_num = df_num.iloc[:, 0:5]\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Cancer Dataset Pairplot\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nCreate a scatterplot of concavity_worst and concave points_worst that colors points based on the 'diagnosis' column, using color =\"bright\".\n\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"radius_mean\", y=\"texture_mean\", color=\"blue\").set(title=\"Radius Mean vs Texture Mean\", xlabel=\"Radius Mean\", ylabel=\"Texture Mean\")\n\nsns.jointplot(data=df, x=\"area_mean\", y=\"smoothness_mean\", color=\"green\", height=6).set_axis_labels(\"Area Mean\", \"Smoothness Mean\").fig.suptitle(\"Area Mean vs Smoothness Mean\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ndf_num = df_num.iloc[:, 0:5]\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Cancer Dataset Pairplot\")\n\n", "code": "sns.scatterplot(data=df, x=\"concavity_worst\", y=\"concave points_worst\", hue=\"diagnosis\", palette=\"bright\").set(title=\"Diagnosis-based Scatterplot\", xlabel=\"Concavity Worst\", ylabel=\"Concave Points Worst\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nCreate a scatterplot with a regression line to visualize the relationship between 'area_mean' and 'perimeter_mean', using color = \"plum\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"radius_mean\", y=\"texture_mean\", color=\"blue\").set(title=\"Radius Mean vs Texture Mean\", xlabel=\"Radius Mean\", ylabel=\"Texture Mean\")\n\nsns.jointplot(data=df, x=\"area_mean\", y=\"smoothness_mean\", color=\"green\", height=6).set_axis_labels(\"Area Mean\", \"Smoothness Mean\").fig.suptitle(\"Area Mean vs Smoothness Mean\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ndf_num = df_num.iloc[:, 0:5]\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Cancer Dataset Pairplot\")\n\nsns.scatterplot(data=df, x=\"concavity_worst\", y=\"concave points_worst\", hue=\"diagnosis\", palette=\"bright\").set(title=\"Diagnosis-based Scatterplot\", xlabel=\"Concavity Worst\", ylabel=\"Concave Points Worst\")\n\n", "code": "sns.regplot(data=df, x=\"area_mean\", y=\"perimeter_mean\", color=\"plum\").set(title=\"Area Mean vs Perimeter Mean\", xlabel=\"Area Mean\", ylabel=\"Perimeter Mean\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "File Path: \"data/lightgbm_dataset05.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/lightgbm_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nSplit the data into features and target \"output\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nDefine a LightGBM model  with max_depth=4, n_estimators=120，learning_rate=0.01 and num_leaves=31. Train the model with Evaluation Metric='logloss'.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "import lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nPredict the target for the test set and Evaluate the model using the test set. Give the confusion matrix and corresponding accuracy, precision, and recall. Remember you only need to display the accuracy of the model on the test set(Keep to two decimal places).\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n", "library": ["LightGBM", "sklearn"], "exlib": ["LightGBM", "sklearn"]}
{"prompt": "\n\nGet the feature importance of each feature，print the importance of the most important feature(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n", "code": "# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nConduct model parameter tuning for max_depth, learning_rate, n_estimators, select three alternative values of each parameter and output the optimal value of n_estimators.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n\n", "code": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [50, 100, 150],\n}\ngrid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_[\"n_estimators\"]\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "File Path: `data/atplotlib_dataset06.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a line plot of \"MEDV\", using figsize=(10,6), color='blue'.\n    \n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['MEDV'], color='blue')\nplt.title(\"Line plot of MEDV\")\nplt.xlabel(\" House Index\")\nplt.ylabel(\"MEDV\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a histogram of the CRIM, using figsize=(10,6), bins=30, color='green', alpha=0.7. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['MEDV'], color='blue')\nplt.title(\"Line plot of MEDV\")\nplt.xlabel(\" House Index\")\nplt.ylabel(\"MEDV\")\nplt.show()\n\n", "code": "plt.figure(figsize=(10,6))\nplt.hist(df['CRIM'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of CRIM\")\nplt.xlabel(\"CRIM\")\nplt.ylabel(\"Frequency\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a scatter graph of the relationship between \"NOX\" and \"RM\" columns.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['MEDV'], color='blue')\nplt.title(\"Line plot of MEDV\")\nplt.xlabel(\" House Index\")\nplt.ylabel(\"MEDV\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['CRIM'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of CRIM\")\nplt.xlabel(\"CRIM\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n", "code": "plt.scatter(df['NOX'], df['RM'])\nplt.title('Relationship between NOX and RM')\nplt.xlabel(\"NOX\")\nplt.ylabel(\"RM\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a pie chart of the unique values of \"CHAS\", using figsize=(8,8).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['MEDV'], color='blue')\nplt.title(\"Line plot of MEDV\")\nplt.xlabel(\" House Index\")\nplt.ylabel(\"MEDV\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['CRIM'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of CRIM\")\nplt.xlabel(\"CRIM\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['NOX'], df['RM'])\nplt.title('Relationship between NOX and RM')\nplt.xlabel(\"NOX\")\nplt.ylabel(\"RM\")\nplt.show()\n\n", "code": "pie_data = df['CHAS'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of CHAS\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nGroup by CHAS and visualize \"RAD\" and \"TAX\" content of each CHAS using a stacked bar chart.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['MEDV'], color='blue')\nplt.title(\"Line plot of MEDV\")\nplt.xlabel(\" House Index\")\nplt.ylabel(\"MEDV\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['CRIM'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of CRIM\")\nplt.xlabel(\"CRIM\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['NOX'], df['RM'])\nplt.title('Relationship between NOX and RM')\nplt.xlabel(\"NOX\")\nplt.ylabel(\"RM\")\nplt.show()\n\npie_data = df['CHAS'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of CHAS\")\nplt.show()\n\n", "code": "\ngrouped_data = df.groupby('CHAS')[['RAD', 'TAX']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('RAD and TAX by CHAS')\nplt.xlabel('CHAS')\nplt.ylabel('Average Content')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a heatmap of the correlation between all the nemerical columns of the DataFrame. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['MEDV'], color='blue')\nplt.title(\"Line plot of MEDV\")\nplt.xlabel(\" House Index\")\nplt.ylabel(\"MEDV\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['CRIM'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of CRIM\")\nplt.xlabel(\"CRIM\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['NOX'], df['RM'])\nplt.title('Relationship between NOX and RM')\nplt.xlabel(\"NOX\")\nplt.ylabel(\"RM\")\nplt.show()\n\npie_data = df['CHAS'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of CHAS\")\nplt.show()\n\n\ngrouped_data = df.groupby('CHAS')[['RAD', 'TAX']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('RAD and TAX by CHAS')\nplt.xlabel('CHAS')\nplt.ylabel('Average Content')\nplt.show()\n\n", "code": "# Select all the numerical columns\ndf = df.select_dtypes(include=['float64', 'int64'])\ncorr = df.corr()\nplt.imshow(corr, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation=90)\nplt.yticks(range(len(corr)), corr.columns)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "File Path: `data/matplotlib_dataset05.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a line plot of \"mean radius\", using figsize=(10,6), color='blue'.\n    \n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['mean radius'], color='blue')\nplt.title(\"Line plot of mean radius\")\nplt.xlabel(\" Cancer Index\")\nplt.ylabel(\"mean radius\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a histogram of the mean texture, using figsize=(10,6), bins=30, color='green', alpha=0.7. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['mean radius'], color='blue')\nplt.title(\"Line plot of mean radius\")\nplt.xlabel(\" Cancer Index\")\nplt.ylabel(\"mean radius\")\nplt.show()\n\n", "code": "plt.figure(figsize=(10,6))\nplt.hist(df['mean texture'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of mean texture\")\nplt.xlabel(\"mean texture\")\nplt.ylabel(\"Frequency\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a scatter graph of the relationship between 'mean area' and 'mean smoothness' columns.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['mean radius'], color='blue')\nplt.title(\"Line plot of mean radius\")\nplt.xlabel(\" Cancer Index\")\nplt.ylabel(\"mean radius\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['mean texture'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of mean texture\")\nplt.xlabel(\"mean texture\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n", "code": "plt.scatter(df['mean area'], df['mean smoothness'])\nplt.title('Relationship between mean area and mean smoothness')\nplt.xlabel(\"mean area\")\nplt.ylabel(\"mean smoothness\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a pie chart of the unique values of \"diagnosis\", using figsize=(8,8).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['mean radius'], color='blue')\nplt.title(\"Line plot of mean radius\")\nplt.xlabel(\" Cancer Index\")\nplt.ylabel(\"mean radius\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['mean texture'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of mean texture\")\nplt.xlabel(\"mean texture\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['mean area'], df['mean smoothness'])\nplt.title('Relationship between mean area and mean smoothness')\nplt.xlabel(\"mean area\")\nplt.ylabel(\"mean smoothness\")\nplt.show()\n\n", "code": "pie_data = df['diagnosis'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of diagnosis\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nGroup by diagnosis and visualize mean compactness and mean concavity content of each diagnosis using a stacked bar chart.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['mean radius'], color='blue')\nplt.title(\"Line plot of mean radius\")\nplt.xlabel(\" Cancer Index\")\nplt.ylabel(\"mean radius\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['mean texture'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of mean texture\")\nplt.xlabel(\"mean texture\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['mean area'], df['mean smoothness'])\nplt.title('Relationship between mean area and mean smoothness')\nplt.xlabel(\"mean area\")\nplt.ylabel(\"mean smoothness\")\nplt.show()\n\npie_data = df['diagnosis'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of diagnosis\")\nplt.show()\n\n", "code": "\ngrouped_data = df.groupby('diagnosis')[['mean compactness', 'mean concavity']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('mean compactness and mean concavity by diagnosis')\nplt.xlabel('diagnosis')\nplt.ylabel('Average Content')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a heatmap of the correlation between all the nemerical columns of the DataFrame. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset05.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['mean radius'], color='blue')\nplt.title(\"Line plot of mean radius\")\nplt.xlabel(\" Cancer Index\")\nplt.ylabel(\"mean radius\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['mean texture'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of mean texture\")\nplt.xlabel(\"mean texture\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['mean area'], df['mean smoothness'])\nplt.title('Relationship between mean area and mean smoothness')\nplt.xlabel(\"mean area\")\nplt.ylabel(\"mean smoothness\")\nplt.show()\n\npie_data = df['diagnosis'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of diagnosis\")\nplt.show()\n\n\ngrouped_data = df.groupby('diagnosis')[['mean compactness', 'mean concavity']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('mean compactness and mean concavity by diagnosis')\nplt.xlabel('diagnosis')\nplt.ylabel('Average Content')\nplt.show()\n\n", "code": "# Select all the numerical columns\ndf = df.select_dtypes(include=['float64', 'int64'])\ncorr = df.corr()\nplt.imshow(corr, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation=90)\nplt.yticks(range(len(corr)), corr.columns)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "File Path: `data/matplotlib_dataset03.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a line plot of Culmen Length (mm), using figsize=(10,6), color='blue'.\n    \n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['Culmen Length (mm)'], color='blue')\nplt.title(\"Line plot of Culmen Length (mm)\")\nplt.xlabel(\"Penguins Index\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a histogram of the Flipper Length (mm), using figsize=(10,6), bins=30, color='green', alpha=0.7. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['Culmen Length (mm)'], color='blue')\nplt.title(\"Line plot of Culmen Length (mm)\")\nplt.xlabel(\"Penguins Index\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n\n", "code": "plt.figure(figsize=(10,6))\nplt.hist(df['Flipper Length (mm)'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a scatter graph of the relationship between Flipper Length (mm) and Body Mass (g) columns.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['Culmen Length (mm)'], color='blue')\nplt.title(\"Line plot of Culmen Length (mm)\")\nplt.xlabel(\"Penguins Index\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['Flipper Length (mm)'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n", "code": "plt.scatter(df['Flipper Length (mm)'], df['Body Mass (g)'])\nplt.title('Relationship between Flipper Length (mm) and Body Mass (g)')\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a pie chart of the unique values of \"Species\", using figsize=(8,8).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['Culmen Length (mm)'], color='blue')\nplt.title(\"Line plot of Culmen Length (mm)\")\nplt.xlabel(\"Penguins Index\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['Flipper Length (mm)'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['Flipper Length (mm)'], df['Body Mass (g)'])\nplt.title('Relationship between Flipper Length (mm) and Body Mass (g)')\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()\n\n", "code": "pie_data = df['Species'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Species\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nGroup by Species and visualize Flipper Length (mm) and Body Mass (g) content of each Specie using a stacked bar chart.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['Culmen Length (mm)'], color='blue')\nplt.title(\"Line plot of Culmen Length (mm)\")\nplt.xlabel(\"Penguins Index\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['Flipper Length (mm)'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['Flipper Length (mm)'], df['Body Mass (g)'])\nplt.title('Relationship between Flipper Length (mm) and Body Mass (g)')\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()\n\npie_data = df['Species'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Species\")\nplt.show()\n\n", "code": "\ngrouped_data = df.groupby('Species')[['Flipper Length (mm)', 'Body Mass (g)']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('Flipper Length (mm) and Body Mass (g) by Specie')\nplt.xlabel('Specie')\nplt.ylabel('Average Content')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a heatmap of the correlation between all the nemerical columns of the DataFrame. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['Culmen Length (mm)'], color='blue')\nplt.title(\"Line plot of Culmen Length (mm)\")\nplt.xlabel(\"Penguins Index\")\nplt.ylabel(\"Culmen Length (mm)\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['Flipper Length (mm)'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Flipper Length (mm)\")\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['Flipper Length (mm)'], df['Body Mass (g)'])\nplt.title('Relationship between Flipper Length (mm) and Body Mass (g)')\nplt.xlabel(\"Flipper Length (mm)\")\nplt.ylabel(\"Body Mass (g)\")\nplt.show()\n\npie_data = df['Species'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Species\")\nplt.show()\n\n\ngrouped_data = df.groupby('Species')[['Flipper Length (mm)', 'Body Mass (g)']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('Flipper Length (mm) and Body Mass (g) by Specie')\nplt.xlabel('Specie')\nplt.ylabel('Average Content')\nplt.show()\n\n", "code": "# Select all the numerical columns\ndf = df.select_dtypes(include=['float64', 'int64'])\ncorr = df.corr()\nplt.imshow(corr, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation=90)\nplt.yticks(range(len(corr)), corr.columns)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "File Path: \"data/seaborn_dataset01.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a scatter plot for sepal length against petal length, using color = \"red\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import seaborn as sns\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", color=\"red\").set(title=\"sepal length against petal length\", xlabel=\"SepalLengthCm\", ylabel=\"PetalLengthCm\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a joint plot for sepal width and petal width, using height = 6, color = \"green\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", color=\"red\").set(title=\"sepal length against petal length\", xlabel=\"SepalLengthCm\", ylabel=\"PetalLengthCm\")\n\n", "code": "sns.jointplot(data=df, x=\"SepalWidthCm\", y=\"PetalWidthCm\", color=\"green\", height=6).set_axis_labels(\"Sepal Width (cm)\", \"Petal Width (cm)\").fig.suptitle(\"Sepal vs Petal Width\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a pair plot for all the numerical columns, using color = \"pastel\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", color=\"red\").set(title=\"sepal length against petal length\", xlabel=\"SepalLengthCm\", ylabel=\"PetalLengthCm\")\n\nsns.jointplot(data=df, x=\"SepalWidthCm\", y=\"PetalWidthCm\", color=\"green\", height=6).set_axis_labels(\"Sepal Width (cm)\", \"Petal Width (cm)\").fig.suptitle(\"Sepal vs Petal Width\")\n\n", "code": "# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Iris Dataset Pairplot\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nGenerate a violin plot for petal_width based on different species, using color = \"orange\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", color=\"red\").set(title=\"sepal length against petal length\", xlabel=\"SepalLengthCm\", ylabel=\"PetalLengthCm\")\n\nsns.jointplot(data=df, x=\"SepalWidthCm\", y=\"PetalWidthCm\", color=\"green\", height=6).set_axis_labels(\"Sepal Width (cm)\", \"Petal Width (cm)\").fig.suptitle(\"Sepal vs Petal Width\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Iris Dataset Pairplot\")\n\n", "code": "sns.violinplot(data=df, x=\"Species\", y=\"PetalWidthCm\", color=\"orange\").set(title=\"Violin Plot of Petal Width\", xlabel=\"Species\", ylabel=\"Petal Width (cm)\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nCreate a scatterplot of sepal length and petal length that colors points based on the 'Species' column, using color = \"bright\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", color=\"red\").set(title=\"sepal length against petal length\", xlabel=\"SepalLengthCm\", ylabel=\"PetalLengthCm\")\n\nsns.jointplot(data=df, x=\"SepalWidthCm\", y=\"PetalWidthCm\", color=\"green\", height=6).set_axis_labels(\"Sepal Width (cm)\", \"Petal Width (cm)\").fig.suptitle(\"Sepal vs Petal Width\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Iris Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"Species\", y=\"PetalWidthCm\", color=\"orange\").set(title=\"Violin Plot of Petal Width\", xlabel=\"Species\", ylabel=\"Petal Width (cm)\")\n\n\n", "code": "sns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", hue=\"Species\", palette=\"bright\").set(title=\"Species-based Scatterplot\", xlabel=\"Sepal Length (cm)\", ylabel=\"Petal Length (cm)\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nCreate a scatterplot with a regression line to visualize the relationship between petal_width and sepal_width, using color = \"plum\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", color=\"red\").set(title=\"sepal length against petal length\", xlabel=\"SepalLengthCm\", ylabel=\"PetalLengthCm\")\n\nsns.jointplot(data=df, x=\"SepalWidthCm\", y=\"PetalWidthCm\", color=\"green\", height=6).set_axis_labels(\"Sepal Width (cm)\", \"Petal Width (cm)\").fig.suptitle(\"Sepal vs Petal Width\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Iris Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"Species\", y=\"PetalWidthCm\", color=\"orange\").set(title=\"Violin Plot of Petal Width\", xlabel=\"Species\", ylabel=\"Petal Width (cm)\")\n\n\nsns.scatterplot(data=df, x=\"SepalLengthCm\", y=\"PetalLengthCm\", hue=\"Species\", palette=\"bright\").set(title=\"Species-based Scatterplot\", xlabel=\"Sepal Length (cm)\", ylabel=\"Petal Length (cm)\")\n\n", "code": "sns.regplot(data=df, x=\"PetalWidthCm\", y=\"SepalWidthCm\", color=\"plum\").set(title=\"Petal Width vs Sepal Width\", xlabel=\"Petal Width (cm)\", ylabel=\"Sepal Width (cm)\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "File Path: \"data/lightgbm_dataset03.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/lightgbm_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nimport re\n# add to solve speical json character format error\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nSplit the data into features and target \"diagnosis\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nimport re\n# add to solve speical json character format error\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['diagnosis'], axis=1)\ny = df['diagnosis']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nDefine a LightGBM model  with max_depth=4, n_estimators=120，learning_rate=0.01 and num_leaves=31. Train the model with Evaluation Metric='logloss'.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nimport re\n# add to solve speical json character format error\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['diagnosis'], axis=1)\ny = df['diagnosis']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "import lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nPredict the target for the test set and Evaluate the model using the test set. Give the confusion matrix and corresponding accuracy, precision, and recall. Remember you only need to display the accuracy of the model on the test set(Keep to two decimal places).\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nimport re\n# add to solve speical json character format error\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['diagnosis'], axis=1)\ny = df['diagnosis']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n", "library": ["LightGBM", "sklearn"], "exlib": ["LightGBM", "sklearn"]}
{"prompt": "\n\nGet the feature importance of each feature，print the importance of the most important feature(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nimport re\n# add to solve speical json character format error\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['diagnosis'], axis=1)\ny = df['diagnosis']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n", "code": "# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nConduct model parameter tuning for max_depth, learning_rate, n_estimators, select three alternative values of each parameter and output the optimal value of n_estimators.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nimport re\n# add to solve speical json character format error\ndf = df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['diagnosis'], axis=1)\ny = df['diagnosis']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n\n", "code": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [50, 100, 150],\n}\ngrid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_[\"n_estimators\"]\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "File Path: `data/matplotlib_dataset01.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a line plot of acceleration, using figsize=(10,6), color='blue'.\n    \n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['acceleration'], color='blue')\nplt.title(\"Line plot of Acceleration\")\nplt.xlabel(\"Car Index\")\nplt.ylabel(\"Acceleration\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a histogram of the weight, using figsize=(10,6), bins=30, color='green', alpha=0.7. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['acceleration'], color='blue')\nplt.title(\"Line plot of Acceleration\")\nplt.xlabel(\"Car Index\")\nplt.ylabel(\"Acceleration\")\nplt.show()\n\n", "code": "plt.figure(figsize=(10,6))\nplt.hist(df['weight'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Weight\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Frequency\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a scatter graph of the relationship between mpg and displacement columns.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['acceleration'], color='blue')\nplt.title(\"Line plot of Acceleration\")\nplt.xlabel(\"Car Index\")\nplt.ylabel(\"Acceleration\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['weight'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Weight\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n", "code": "plt.scatter(df['mpg'], df['displacement'])\nplt.title('Relationship between mpg and displacement')\nplt.xlabel('mpg')\nplt.ylabel('displacement')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a pie chart of the unique values of \"origin\", using figsize=(8,8).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['acceleration'], color='blue')\nplt.title(\"Line plot of Acceleration\")\nplt.xlabel(\"Car Index\")\nplt.ylabel(\"Acceleration\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['weight'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Weight\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['mpg'], df['displacement'])\nplt.title('Relationship between mpg and displacement')\nplt.xlabel('mpg')\nplt.ylabel('displacement')\nplt.show()\n\n", "code": "pie_data = df['origin'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Origin\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nGroup by origin and visualize horsepower and weight content of each origin using a stacked bar chart.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['acceleration'], color='blue')\nplt.title(\"Line plot of Acceleration\")\nplt.xlabel(\"Car Index\")\nplt.ylabel(\"Acceleration\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['weight'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Weight\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['mpg'], df['displacement'])\nplt.title('Relationship between mpg and displacement')\nplt.xlabel('mpg')\nplt.ylabel('displacement')\nplt.show()\n\npie_data = df['origin'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Origin\")\nplt.show()\n\n", "code": "\ngrouped_data = df.groupby('origin')[['horsepower', 'weight']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('Horsepower and Weight Content by Origin')\nplt.xlabel('Origin')\nplt.ylabel('Average Content')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a heatmap of the correlation between all the nemerical columns of the DataFrame. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset01.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['acceleration'], color='blue')\nplt.title(\"Line plot of Acceleration\")\nplt.xlabel(\"Car Index\")\nplt.ylabel(\"Acceleration\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['weight'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Weight\")\nplt.xlabel(\"Weight\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['mpg'], df['displacement'])\nplt.title('Relationship between mpg and displacement')\nplt.xlabel('mpg')\nplt.ylabel('displacement')\nplt.show()\n\npie_data = df['origin'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Origin\")\nplt.show()\n\n\ngrouped_data = df.groupby('origin')[['horsepower', 'weight']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('Horsepower and Weight Content by Origin')\nplt.xlabel('Origin')\nplt.ylabel('Average Content')\nplt.show()\n\n", "code": "# Select all the numerical columns\ndf = df.select_dtypes(include=['float64', 'int64'])\ncorr = df.corr()\nplt.imshow(corr, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation=90)\nplt.yticks(range(len(corr)), corr.columns)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "File Path: \"data/lightgbm_dataset02.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame using sep = \";\". Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/lightgbm_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nSplit the data into features and target \"quality\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['quality'], axis=1)\ny = df['quality']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nDefine a LightGBM model  with max_depth=4, n_estimators=120，learning_rate=0.01 and num_leaves=31. Train the model with Evaluation Metric='logloss'.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['quality'], axis=1)\ny = df['quality']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "import lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nPredict the target for the test set and Evaluate the model using the test set. Give the confusion matrix and corresponding accuracy, precision, and recall. Remember you only need to display the accuracy of the model on the test set(Keep to two decimal places).\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['quality'], axis=1)\ny = df['quality']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n", "library": ["LightGBM", "sklearn"], "exlib": ["LightGBM", "sklearn"]}
{"prompt": "\n\nGet the feature importance of each feature，print the importance of the most important feature(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['quality'], axis=1)\ny = df['quality']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n", "code": "# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nConduct model parameter tuning for max_depth, learning_rate, n_estimators, select three alternative values of each parameter and output the optimal value of n_estimators.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['quality'], axis=1)\ny = df['quality']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n\n", "code": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [50, 100, 150],\n}\ngrid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_[\"n_estimators\"]\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "File Path: `data/matplotlib_dataset04.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a line plot of \"age\", using figsize=(10,6), color='blue'.\n    \n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['age'], color='blue')\nplt.title(\"Line plot of age\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"age\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a histogram of the chol, using figsize=(10,6), bins=30, color='green', alpha=0.7. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['age'], color='blue')\nplt.title(\"Line plot of age\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"age\")\nplt.show()\n\n", "code": "plt.figure(figsize=(10,6))\nplt.hist(df['chol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of chol\")\nplt.xlabel(\"chol\")\nplt.ylabel(\"Frequency\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a scatter graph of the relationship between \"maximum heart rate\" and \"age\" columns.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['age'], color='blue')\nplt.title(\"Line plot of age\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"age\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['chol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of chol\")\nplt.xlabel(\"chol\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n", "code": "plt.scatter(df['maximum heart rate'], df['age'])\nplt.title('Relationship between maximum heart rate and age')\nplt.xlabel(\"maximum heart rate\")\nplt.ylabel(\"age\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a pie chart of the unique values of \"presence of heart disease\", using figsize=(8,8).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['age'], color='blue')\nplt.title(\"Line plot of age\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"age\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['chol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of chol\")\nplt.xlabel(\"chol\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['maximum heart rate'], df['age'])\nplt.title('Relationship between maximum heart rate and age')\nplt.xlabel(\"maximum heart rate\")\nplt.ylabel(\"age\")\nplt.show()\n\n", "code": "pie_data = df['presence of heart disease'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of presence of heart disease\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nGroup by presence of heart disease and visualize blood pressure and chol content of each presence of heart disease using a stacked bar chart.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['age'], color='blue')\nplt.title(\"Line plot of age\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"age\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['chol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of chol\")\nplt.xlabel(\"chol\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['maximum heart rate'], df['age'])\nplt.title('Relationship between maximum heart rate and age')\nplt.xlabel(\"maximum heart rate\")\nplt.ylabel(\"age\")\nplt.show()\n\npie_data = df['presence of heart disease'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of presence of heart disease\")\nplt.show()\n\n", "code": "\ngrouped_data = df.groupby('presence of heart disease')[['blood pressure', 'chol']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('blood pressure and chol by presence of heart disease')\nplt.xlabel('presence of heart disease')\nplt.ylabel('Average Content')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a heatmap of the correlation between all the nemerical columns of the DataFrame. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['age'], color='blue')\nplt.title(\"Line plot of age\")\nplt.xlabel(\"Index\")\nplt.ylabel(\"age\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['chol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of chol\")\nplt.xlabel(\"chol\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['maximum heart rate'], df['age'])\nplt.title('Relationship between maximum heart rate and age')\nplt.xlabel(\"maximum heart rate\")\nplt.ylabel(\"age\")\nplt.show()\n\npie_data = df['presence of heart disease'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of presence of heart disease\")\nplt.show()\n\n\ngrouped_data = df.groupby('presence of heart disease')[['blood pressure', 'chol']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('blood pressure and chol by presence of heart disease')\nplt.xlabel('presence of heart disease')\nplt.ylabel('Average Content')\nplt.show()\n\n", "code": "# Select all the numerical columns\ndf = df.select_dtypes(include=['float64', 'int64'])\ncorr = df.corr()\nplt.imshow(corr, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation=90)\nplt.yticks(range(len(corr)), corr.columns)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "File Path: \"data/lightgbm_dataset01.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/lightgbm_dataset01.csv\"\niris_df = pd.read_csv(path)\nprint(iris_df.columns)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nSplit the data into features and target \"species\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset01.csv\"\niris_df = pd.read_csv(path)\nprint(iris_df.columns)\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = iris_df.drop(columns=['Species'], axis=1)\ny = iris_df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nDefine a LightGBM model  with max_depth=4, n_estimators=120，learning_rate=0.01 and num_leaves=31. Train the model with Evaluation Metric='logloss'.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset01.csv\"\niris_df = pd.read_csv(path)\nprint(iris_df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = iris_df.drop(columns=['Species'], axis=1)\ny = iris_df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "import lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nPredict the target for the test set and Evaluate the model using the test set. Give the confusion matrix and corresponding accuracy, precision, and recall. Remember you only need to display the accuracy of the model on the test set(Keep to two decimal places).\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset01.csv\"\niris_df = pd.read_csv(path)\nprint(iris_df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = iris_df.drop(columns=['Species'], axis=1)\ny = iris_df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n", "library": ["LightGBM", "sklearn"], "exlib": ["LightGBM", "sklearn"]}
{"prompt": "\n\nGet the feature importance of each feature，print the importance of the most important feature(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset01.csv\"\niris_df = pd.read_csv(path)\nprint(iris_df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = iris_df.drop(columns=['Species'], axis=1)\ny = iris_df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n", "code": "# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nConduct model parameter tuning for max_depth, learning_rate, n_estimators, select three alternative values of each parameter and output the optimal value of n_estimators.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset01.csv\"\niris_df = pd.read_csv(path)\nprint(iris_df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = iris_df.drop(columns=['Species'], axis=1)\ny = iris_df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n\n", "code": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [50, 100, 150],\n}\ngrid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_[\"n_estimators\"]\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "File Path: \"data/seaborn_dataset03.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a scatter plot for 'weight' against 'mpg', using color = \"blue\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import seaborn as sns\nsns.scatterplot(data=df, x=\"weight\", y=\"mpg\", color=\"blue\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a joint plot for 'horsepower' and 'acceleration', using height = 6, color = \"green\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"weight\", y=\"mpg\", color=\"blue\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n\n", "code": "sns.jointplot(data=df, x=\"horsepower\", y=\"acceleration\", color=\"green\", height=6).set_axis_labels(\"Horsepower\", \"Acceleration\").fig.suptitle(\"Horsepower vs Acceleration\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a pair plot for all the numerical columns, using color = \"pastel\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"weight\", y=\"mpg\", color=\"blue\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n\nsns.jointplot(data=df, x=\"horsepower\", y=\"acceleration\", color=\"green\", height=6).set_axis_labels(\"Horsepower\", \"Acceleration\").fig.suptitle(\"Horsepower vs Acceleration\")\n\n", "code": "# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Auto MPG Dataset Pairplot\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a violin plot for model year based on different origin, using color = \"orange\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"weight\", y=\"mpg\", color=\"blue\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n\nsns.jointplot(data=df, x=\"horsepower\", y=\"acceleration\", color=\"green\", height=6).set_axis_labels(\"Horsepower\", \"Acceleration\").fig.suptitle(\"Horsepower vs Acceleration\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Auto MPG Dataset Pairplot\")\n\n", "code": "sns.violinplot(data=df, x=\"origin\", y=\"model year\", color=\"orange\").set(title=\"Violin Plot of model year\", xlabel=\"origin\", ylabel=\"model year\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nCreate a scatterplot of displacement and mpg that colors points based on the 'origin' column, using color = \"bright\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"weight\", y=\"mpg\", color=\"blue\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n\nsns.jointplot(data=df, x=\"horsepower\", y=\"acceleration\", color=\"green\", height=6).set_axis_labels(\"Horsepower\", \"Acceleration\").fig.suptitle(\"Horsepower vs Acceleration\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Auto MPG Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"origin\", y=\"model year\", color=\"orange\").set(title=\"Violin Plot of model year\", xlabel=\"origin\", ylabel=\"model year\")\n\n\n", "code": "sns.scatterplot(data=df, x=\"displacement\", y=\"mpg\", hue=\"origin\", palette=\"bright\").set(title=\"origin-based Scatterplot\", xlabel=\"Displacement\", ylabel=\"MPG\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nCreate a scatterplot with a regression line to visualize the relationship between 'weight' and 'mpg', using color = \"plum\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset03.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"weight\", y=\"mpg\", color=\"blue\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n\nsns.jointplot(data=df, x=\"horsepower\", y=\"acceleration\", color=\"green\", height=6).set_axis_labels(\"Horsepower\", \"Acceleration\").fig.suptitle(\"Horsepower vs Acceleration\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Auto MPG Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"origin\", y=\"model year\", color=\"orange\").set(title=\"Violin Plot of model year\", xlabel=\"origin\", ylabel=\"model year\")\n\n\nsns.scatterplot(data=df, x=\"displacement\", y=\"mpg\", hue=\"origin\", palette=\"bright\").set(title=\"origin-based Scatterplot\", xlabel=\"Displacement\", ylabel=\"MPG\")\n\n", "code": "sns.regplot(data=df, x=\"weight\", y=\"mpg\", color=\"plum\").set(title=\"Weight vs MPG\", xlabel=\"Weight\", ylabel=\"MPG\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "File Path: `data/matplotlib_dataset02.csv`\n\nLoad the dataset from the file path into a pandas DataFrame using sep = \";\". Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a line plot of fixed acidity, using figsize=(10,6), color='blue'.\n    \n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['fixed acidity'], color='blue')\nplt.title(\"Line plot of Fixed Acidity\")\nplt.xlabel(\"Wine Index\")\nplt.ylabel(\"Fixed Acidity\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a histogram of the alcohol, using figsize=(10,6), bins=30, color='green', alpha=0.7. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['fixed acidity'], color='blue')\nplt.title(\"Line plot of Fixed Acidity\")\nplt.xlabel(\"Wine Index\")\nplt.ylabel(\"Fixed Acidity\")\nplt.show()\n\n", "code": "plt.figure(figsize=(10,6))\nplt.hist(df['alcohol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Alcohol Content\")\nplt.xlabel(\"Alcohol Content\")\nplt.ylabel(\"Frequency\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a scatter graph of the relationship between pH and alcohol columns.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['fixed acidity'], color='blue')\nplt.title(\"Line plot of Fixed Acidity\")\nplt.xlabel(\"Wine Index\")\nplt.ylabel(\"Fixed Acidity\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['alcohol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Alcohol Content\")\nplt.xlabel(\"Alcohol Content\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n", "code": "plt.scatter(df['pH'], df['alcohol'])\nplt.title('Relationship between pH and Alcohol')\nplt.xlabel('pH')\nplt.ylabel('Alcohol')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nCreate a pie chart of the unique values of \"quality\", using figsize=(8,8).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['fixed acidity'], color='blue')\nplt.title(\"Line plot of Fixed Acidity\")\nplt.xlabel(\"Wine Index\")\nplt.ylabel(\"Fixed Acidity\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['alcohol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Alcohol Content\")\nplt.xlabel(\"Alcohol Content\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['pH'], df['alcohol'])\nplt.title('Relationship between pH and Alcohol')\nplt.xlabel('pH')\nplt.ylabel('Alcohol')\nplt.show()\n\n", "code": "pie_data = df['quality'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Wine Quality\")\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nGroup by quality and visualize fixed acidity and alcohol content of each quality using a stacked bar chart.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['fixed acidity'], color='blue')\nplt.title(\"Line plot of Fixed Acidity\")\nplt.xlabel(\"Wine Index\")\nplt.ylabel(\"Fixed Acidity\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['alcohol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Alcohol Content\")\nplt.xlabel(\"Alcohol Content\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['pH'], df['alcohol'])\nplt.title('Relationship between pH and Alcohol')\nplt.xlabel('pH')\nplt.ylabel('Alcohol')\nplt.show()\n\npie_data = df['quality'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Wine Quality\")\nplt.show()\n\n", "code": "\ngrouped_data = df.groupby('quality')[['fixed acidity', 'alcohol']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('Fixed Acidity and Alcohol Content by Quality')\nplt.xlabel('Quality')\nplt.ylabel('Average Content')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nDraw a heatmap of the correlation between all the nemerical columns of the DataFrame. \n\n", "pre_code": "import pandas as pd\n\npath = \"data/matplotlib_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10,6))\nplt.plot(df['fixed acidity'], color='blue')\nplt.title(\"Line plot of Fixed Acidity\")\nplt.xlabel(\"Wine Index\")\nplt.ylabel(\"Fixed Acidity\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nplt.hist(df['alcohol'], bins=30, color='green', alpha=0.7)\nplt.title(\"Histogram of Alcohol Content\")\nplt.xlabel(\"Alcohol Content\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.scatter(df['pH'], df['alcohol'])\nplt.title('Relationship between pH and Alcohol')\nplt.xlabel('pH')\nplt.ylabel('Alcohol')\nplt.show()\n\npie_data = df['quality'].value_counts()\nplt.figure(figsize=(8,8))\nplt.pie(pie_data, labels=pie_data.index, autopct='%1.1f%%')\nplt.title(\"Pie chart of Wine Quality\")\nplt.show()\n\n\ngrouped_data = df.groupby('quality')[['fixed acidity', 'alcohol']].mean()\n\n# Creating a stacked bar chart\ngrouped_data.plot(kind='bar', stacked=True)\nplt.title('Fixed Acidity and Alcohol Content by Quality')\nplt.xlabel('Quality')\nplt.ylabel('Average Content')\nplt.show()\n\n", "code": "# Select all the numerical columns\ndf = df.select_dtypes(include=['float64', 'int64'])\ncorr = df.corr()\nplt.imshow(corr, cmap='coolwarm', interpolation='nearest')\nplt.colorbar()\nplt.xticks(range(len(corr)), corr.columns, rotation=90)\nplt.yticks(range(len(corr)), corr.columns)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "File Path: \"data/seaborn_dataset06.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a scatter plot for 'rm' against 'medv', using color = \"blue\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import seaborn as sns\nsns.scatterplot(data=df, x=\"rm\", y=\"medv\", color=\"blue\").set(title=\"Rooms vs Median Value\", xlabel=\"Average Number of Rooms\", ylabel=\"Median Value\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a joint plot for 'age' and 'tax', using height = 6, color = \"green\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"rm\", y=\"medv\", color=\"blue\").set(title=\"Rooms vs Median Value\", xlabel=\"Average Number of Rooms\", ylabel=\"Median Value\")\n\n", "code": "sns.jointplot(data=df, x=\"age\", y=\"tax\", color=\"green\", height=6).set_axis_labels(\"Age of Property\", \"Tax Rate\").fig.suptitle(\"Age vs Tax\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a pair plot for all the numerical columns, using color = \"pastel\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"rm\", y=\"medv\", color=\"blue\").set(title=\"Rooms vs Median Value\", xlabel=\"Average Number of Rooms\", ylabel=\"Median Value\")\n\nsns.jointplot(data=df, x=\"age\", y=\"tax\", color=\"green\", height=6).set_axis_labels(\"Age of Property\", \"Tax Rate\").fig.suptitle(\"Age vs Tax\")\n\n", "code": "# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"boston house dataset Pairplot\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a violin plot for 'lstat' based on different chas, using color = \"orange\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"rm\", y=\"medv\", color=\"blue\").set(title=\"Rooms vs Median Value\", xlabel=\"Average Number of Rooms\", ylabel=\"Median Value\")\n\nsns.jointplot(data=df, x=\"age\", y=\"tax\", color=\"green\", height=6).set_axis_labels(\"Age of Property\", \"Tax Rate\").fig.suptitle(\"Age vs Tax\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"boston house dataset Pairplot\")\n\n", "code": "sns.violinplot(data=df, x=\"chas\", y=\"lstat\", color=\"orange\").set(title=\"Violin Plot of lstat\", xlabel=\"chas\", ylabel=\"lstat\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nCreate a scatterplot of nox and medv that colors points based on the 'chas' column, using color = \"bright\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"rm\", y=\"medv\", color=\"blue\").set(title=\"Rooms vs Median Value\", xlabel=\"Average Number of Rooms\", ylabel=\"Median Value\")\n\nsns.jointplot(data=df, x=\"age\", y=\"tax\", color=\"green\", height=6).set_axis_labels(\"Age of Property\", \"Tax Rate\").fig.suptitle(\"Age vs Tax\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"boston house dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"chas\", y=\"lstat\", color=\"orange\").set(title=\"Violin Plot of lstat\", xlabel=\"chas\", ylabel=\"lstat\")\n\n\n", "code": "sns.scatterplot(data=df, x=\"nox\", y=\"medv\", hue=\"chas\", palette=\"bright\").set(title=\"chas-based Scatterplot\", xlabel=\"nox\", ylabel=\"medv\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nCreate a scatterplot with a regression line to visualize the relationship between 'b' and 'medv', using color = \"plum\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset06.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"rm\", y=\"medv\", color=\"blue\").set(title=\"Rooms vs Median Value\", xlabel=\"Average Number of Rooms\", ylabel=\"Median Value\")\n\nsns.jointplot(data=df, x=\"age\", y=\"tax\", color=\"green\", height=6).set_axis_labels(\"Age of Property\", \"Tax Rate\").fig.suptitle(\"Age vs Tax\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"boston house dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"chas\", y=\"lstat\", color=\"orange\").set(title=\"Violin Plot of lstat\", xlabel=\"chas\", ylabel=\"lstat\")\n\n\nsns.scatterplot(data=df, x=\"nox\", y=\"medv\", hue=\"chas\", palette=\"bright\").set(title=\"chas-based Scatterplot\", xlabel=\"nox\", ylabel=\"medv\")\n\n", "code": "sns.regplot(data=df, x=\"b\", y=\"medv\", color=\"plum\").set(title=\"Proportion of Blacks vs Median Value\", xlabel=\"Proportion of Blacks by Town\", ylabel=\"Median Value\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "File Path: \"data/lightgbm_dataset04.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/lightgbm_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nSplit the data into features and target \"Outcome\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nDefine a LightGBM model  with max_depth=4, n_estimators=120，learning_rate=0.01 and num_leaves=31. Train the model with Evaluation Metric='logloss'.\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "import lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nPredict the target for the test set and Evaluate the model using the test set. Give the confusion matrix and corresponding accuracy, precision, and recall. Remember you only need to display the accuracy of the model on the test set(Keep to two decimal places).\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n", "library": ["LightGBM", "sklearn"], "exlib": ["LightGBM", "sklearn"]}
{"prompt": "\n\nGet the feature importance of each feature，print the importance of the most important feature(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n", "code": "# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n", "library": ["LightGBM"], "exlib": ["LightGBM"]}
{"prompt": "\n\nConduct model parameter tuning for max_depth, learning_rate, n_estimators, select three alternative values of each parameter and output the optimal value of n_estimators.\n\n", "pre_code": "import pandas as pd\n\npath = \"data/lightgbm_dataset04.csv\"\ndf = pd.read_csv(path)\nprint(df.columns)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nimport lightgbm as lgb\n\nlgbm_model = lgb.LGBMClassifier(max_depth=4, n_estimators=120, learning_rate=0.01, num_leaves=31)\nlgbm_model.fit(X_train, y_train, eval_metric='logloss', eval_set=[(X_test, y_test)])\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n\ny_pred = lgbm_model.predict(X_test)\nconf_matrix = confusion_matrix(y_test, y_pred)\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='macro')\nrecall = recall_score(y_test, y_pred, average='macro')\nround(accuracy, 2)\n\n# 获取特征重要性\nimportance = lgbm_model.feature_importances_\nfeature_names = X_train.columns\nfeature_importance = pd.DataFrame({'feature_names': feature_names, 'importance': importance})\nfeature_importance = feature_importance.sort_values(by='importance', ascending=False)\nround(feature_importance.iloc[0][1], 2)\n\n", "code": "from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'max_depth': [3, 4, 5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'n_estimators': [50, 100, 150],\n}\ngrid_search = GridSearchCV(estimator=lgbm_model, param_grid=param_grid, scoring='accuracy', cv=3, verbose=1)\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_[\"n_estimators\"]\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "File Path: \"data/seaborn_dataset02.csv\"\n\nLoad the dataset from the file path into a pandas DataFrame using sep=';'. Display the column names and the first 5 rows of the DataFrame.\n", "pre_code": "", "code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCreate a scatter plot for 'alcohol' against 'quality', using color = \"red\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\n", "code": "import seaborn as sns\nsns.scatterplot(data=df, x=\"alcohol\", y=\"quality\", color=\"red\").set(title=\"Alcohol vs Quality\", xlabel=\"Alcohol\", ylabel=\"Quality\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a joint plot for 'fixed acidity' and 'pH', using height = 6, color = \"green\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"alcohol\", y=\"quality\", color=\"red\").set(title=\"Alcohol vs Quality\", xlabel=\"Alcohol\", ylabel=\"Quality\")\n\n", "code": "sns.jointplot(data=df, x=\"fixed acidity\", y=\"pH\", color=\"green\", height=6).set_axis_labels(\"Fixed Acidity\", \"pH\").fig.suptitle(\"Fixed Acidity vs pH\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a pair plot for all the numerical columns, using color = \"pastel\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"alcohol\", y=\"quality\", color=\"red\").set(title=\"Alcohol vs Quality\", xlabel=\"Alcohol\", ylabel=\"Quality\")\n\nsns.jointplot(data=df, x=\"fixed acidity\", y=\"pH\", color=\"green\", height=6).set_axis_labels(\"Fixed Acidity\", \"pH\").fig.suptitle(\"Fixed Acidity vs pH\")\n\n", "code": "# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Wine Quality Dataset Pairplot\")\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\n\nGenerate a violin plot for 'sulphates' based on different 'quality', using color = \"orange\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"alcohol\", y=\"quality\", color=\"red\").set(title=\"Alcohol vs Quality\", xlabel=\"Alcohol\", ylabel=\"Quality\")\n\nsns.jointplot(data=df, x=\"fixed acidity\", y=\"pH\", color=\"green\", height=6).set_axis_labels(\"Fixed Acidity\", \"pH\").fig.suptitle(\"Fixed Acidity vs pH\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Wine Quality Dataset Pairplot\")\n\n", "code": "sns.violinplot(data=df, x=\"quality\", y=\"sulphates\", color=\"orange\").set(title=\"Violin Plot of Sulphates\", xlabel=\"Quality\", ylabel=\"Sulphates\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nCreate a scatterplot of alcohol and pH that colors points based on the 'quality' column, using color = \"bright\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"alcohol\", y=\"quality\", color=\"red\").set(title=\"Alcohol vs Quality\", xlabel=\"Alcohol\", ylabel=\"Quality\")\n\nsns.jointplot(data=df, x=\"fixed acidity\", y=\"pH\", color=\"green\", height=6).set_axis_labels(\"Fixed Acidity\", \"pH\").fig.suptitle(\"Fixed Acidity vs pH\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Wine Quality Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"quality\", y=\"sulphates\", color=\"orange\").set(title=\"Violin Plot of Sulphates\", xlabel=\"Quality\", ylabel=\"Sulphates\")\n\n\n", "code": "sns.scatterplot(data=df, x=\"alcohol\", y=\"pH\", hue=\"quality\", palette=\"bright\").set(title=\"Quality-based Scatterplot\", xlabel=\"Alcohol\", ylabel=\"pH\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "\nCreate a scatterplot with a regression line to visualize the relationship between 'volatile acidity' and 'quality', using color = \"plum\".\n", "pre_code": "import pandas as pd\n\npath = \"data/seaborn_dataset02.csv\"\ndf = pd.read_csv(path, sep=';')\nprint(df.columns)\nprint(df.head(5))\n\nimport seaborn as sns\nsns.scatterplot(data=df, x=\"alcohol\", y=\"quality\", color=\"red\").set(title=\"Alcohol vs Quality\", xlabel=\"Alcohol\", ylabel=\"Quality\")\n\nsns.jointplot(data=df, x=\"fixed acidity\", y=\"pH\", color=\"green\", height=6).set_axis_labels(\"Fixed Acidity\", \"pH\").fig.suptitle(\"Fixed Acidity vs pH\")\n\n# select all the numerical columns\ndf_num = df.select_dtypes(include=['float64', 'int64'])\nsns.pairplot(df_num, palette=\"pastel\").fig.suptitle(\"Wine Quality Dataset Pairplot\")\n\nsns.violinplot(data=df, x=\"quality\", y=\"sulphates\", color=\"orange\").set(title=\"Violin Plot of Sulphates\", xlabel=\"Quality\", ylabel=\"Sulphates\")\n\n\nsns.scatterplot(data=df, x=\"alcohol\", y=\"pH\", hue=\"quality\", palette=\"bright\").set(title=\"Quality-based Scatterplot\", xlabel=\"Alcohol\", ylabel=\"pH\")\n\n\n", "code": "sns.regplot(data=df, x=\"volatile acidity\", y=\"quality\", color=\"plum\").set(title=\"Volatile Acidity vs Quality\", xlabel=\"Volatile Acidity\", ylabel=\"Quality\")\n\n", "library": ["seaborn"], "exlib": ["seaborn"]}
{"prompt": "File Path : 'data/tensorflow_dataset01'\n### Load the flower dataset from {data/tensorflow_dataset01}. There are 5 subfolders in the folder, each containing a different number of images, one subfolder representing a category, and the name of the subfolder is the label. You need to divide the images for each category into training sets and test sets in a ratio of 4:1. Normalize the size of each image to (200,200,3) and normalize the pixel values of each image to between 0 and 1.\n", "pre_code": "", "code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset01'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Construct a convolutional neural network model. The size of the input should be {(200,200,3)} and the size of the output should be {5} which represents the number of image categories.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset01'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\n", "code": "from tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Compile the model. Define Adam as optimizer and SparseCategoricalCrossentropy as loss function. Choose 'accuracy' as the metric to be monitored during training and evaluation. Train the model for {5} epochs.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset01'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\n", "code": "model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Draw a plot where the accuracy on the training set and the validation set changes with the number of training epochs. Two lines are displayed on one plot with label 'accuracy' and 'val_accuracy'. Set the x-label as 'Epoch' and y-label as 'Accuracy'. Finally display the plot.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset01'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\n", "code": "import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\n### Evaluate the model. Calculate the result of the test loss plus the test accuracy. Print the result (rounded to two decimal places).\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset01'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n", "code": "test_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Select a test image (image index = 42) for prediction. Display the image in a plot with the True label and the Predicted label in the title of the plot. The format should be \"True label: {true_label}, Predicted label: {predicted_label}\"\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset01'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\ntest_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n\n", "code": "import numpy as np\nfrom tensorflow.keras.preprocessing import image\nimg_index = 2\n\ntest_img_path = validation_generator.filepaths[img_index]\ntest_img = image.load_img(test_img_path, target_size=(200, 200))\ntest_img = image.img_to_array(test_img) / 255.0  # Normalize pixel values to [0, 1]\ntest_img = np.expand_dims(test_img, axis=0)\ntrue_label = validation_generator.classes[img_index]\n\npredictions = model.predict(test_img)\n\nplt.imshow(test_img[0])\nplt.title(f\"True label: {true_label}, Predicted label: {np.argmax(predictions)}\")\nplt.show()\n", "library": ["numpy", "matplotlib", "tensorflow"], "exlib": ["numpy", "matplotlib", "tensorflow"]}
{"prompt": "File Path: 'data/opencv_dataset10.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset10.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset10.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset10.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset10.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset10.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset10.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: `data/sklearn_dataset06.csv`\n\n### Load the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Split the data into features and target \"Year\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Year'], axis=1)\ny = df['Year']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Create a SimpleImputer object, populate NaN values in the dataframe with the mean, and fit transformations on X_train and X_test. First perform dimensionality reduction using \"PCA\" on the feature data ，then display different categories of data in three dimensions.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Year'], axis=1)\ny = df['Year']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "from sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of MonthlyDeaths Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Establish a MLP classifaction model, using optimizer \"Adam\" and activation function \"relu\". Then conduct model training for 200 epochs.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Year'], axis=1)\ny = df['Year']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of MonthlyDeaths Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\n", "code": "from sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Draw a loss curve with epoch as the horizontal axis and loss as the vertical axis.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Year'], axis=1)\ny = df['Year']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of MonthlyDeaths Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\n", "code": "train_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Make predictions on the test set, give the confusion matrix, and count the prediction accuracy, precision, recall, and F1 score of each category. Finally, display the accuracy of the model on the test set (rounded to two decimal places).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Year'], axis=1)\ny = df['Year']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of MonthlyDeaths Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Visualize predictive data with 3D plots.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset06.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Year'], axis=1)\ny = df['Year']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of MonthlyDeaths Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n\n", "code": "predictions = model.predict(X_test)\nX_pca_test = pca.transform(X_test)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=LabelEncoder().fit_transform(predictions))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "File Path: 'data/opencv_dataset08.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset08.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset08.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset08.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset08.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset08.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset08.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: 'data/opencv_dataset05.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset05.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset05.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset05.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset05.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset05.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset05.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: 'data/opencv_dataset03.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset03.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset03.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset03.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset03.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset03.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset03.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: `data/sklearn_dataset04.csv`\n\n### Load the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Split the data into features and target \"Category\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Category'], axis=1)\ny = df['Category']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### First perform dimensionality reduction using \"PCA\" on the feature data ，then display different categories of data in three dimensions.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Category'], axis=1)\ny = df['Category']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Fastfood Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Establish a MLP classifaction model, using optimizer \"Adam\" and activation function \"relu\". Then conduct model training for 200 epochs.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Category'], axis=1)\ny = df['Category']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Fastfood Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\n", "code": "from sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Draw a loss curve with epoch as the horizontal axis and loss as the vertical axis.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Category'], axis=1)\ny = df['Category']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Fastfood Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\n", "code": "train_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Make predictions on the test set, give the confusion matrix, and count the prediction accuracy, precision, recall, and F1 score of each category. Finally, display the accuracy of the model on the test set (rounded to two decimal places).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Category'], axis=1)\ny = df['Category']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Fastfood Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Visualize predictive data with 3D plots.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset04.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Category'], axis=1)\ny = df['Category']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Fastfood Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n\n", "code": "predictions = model.predict(X_test)\nX_pca_test = pca.transform(X_test)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=LabelEncoder().fit_transform(predictions))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "File Path: 'data/opencv_dataset09.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset09.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset09.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset09.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset09.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset09.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset09.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: 'data/opencv_dataset01.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset01.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset01.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset01.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset01.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset01.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset01.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: 'data/opencv_dataset04.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset04.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset04.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset04.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset04.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset04.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset04.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path: `data/sklearn_dataset02.csv`\n\n### Load the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Split the data into features and target \"Outcome\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### First perform dimensionality reduction using \"PCA\" on the feature data ，then display different categories of data in three dimensions.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Diabetes Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Establish a MLP classifaction model, using optimizer \"Adam\" and activation function \"relu\". Then conduct model training for 200 epochs.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Diabetes Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\n", "code": "from sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Draw a loss curve with epoch as the horizontal axis and loss as the vertical axis.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Diabetes Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\n", "code": "train_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\nMake predictions on the test set, give the confusion matrix, and count the prediction accuracy, precision, recall, and F1 score of each category. Finally, display the accuracy of the model on the test set (rounded to two decimal places).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Diabetes Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nVisualize predictive data with 3D plots.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset02.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Outcome'], axis=1)\ny = df['Outcome']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Diabetes Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n\n", "code": "predictions = model.predict(X_test)\nX_pca_test = pca.transform(X_test)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=LabelEncoder().fit_transform(predictions))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "File Path: 'data/opencv_dataset02.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset02.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset02.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset02.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset02.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset02.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corner using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset02.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path : 'data/tensorflow_dataset06'\n### Load the flower dataset from {data/tensorflow_dataset06}. There are 5 subfolders in the folder, each containing a different number of images, one subfolder representing a category, and the name of the subfolder is the label. You need to divide the images for each category into training sets and test sets in a ratio of 4:1. Normalize the size of each image to (200,200,3) and normalize the pixel values of each image to between 0 and 1.\n", "pre_code": "", "code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset06'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Construct a convolutional neural network model. The size of the input should be {(200,200,3)} and the size of the output should be {5} which represents the number of image categories.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset06'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\n", "code": "from tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Compile the model. Define Adam as optimizer and SparseCategoricalCrossentropy as loss function. Choose 'accuracy' as the metric to be monitored during training and evaluation. Train the model for {5} epochs.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset06'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\n", "code": "model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Draw a plot where the accuracy on the training set and the validation set changes with the number of training epochs. Two lines are displayed on one plot with label 'accuracy' and 'val_accuracy'. Set the x-label as 'Epoch' and y-label as 'Accuracy'. Finally display the plot.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset06'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\n", "code": "import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\n### Evaluate the model. Calculate the result of the test loss plus the test accuracy. Print the result (rounded to two decimal places).\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset06'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n", "code": "test_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Select a test image (image index = 12) for prediction. Display the image in a plot with the True label and the Predicted label in the title of the plot. The format should be \"True label: {true_label}, Predicted label: {predicted_label}\"\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset06'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(5, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\ntest_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n\n", "code": "import numpy as np\nfrom tensorflow.keras.preprocessing import image\nimg_index = 2\n\ntest_img_path = validation_generator.filepaths[img_index]\ntest_img = image.load_img(test_img_path, target_size=(200, 200))\ntest_img = image.img_to_array(test_img) / 255.0  # Normalize pixel values to [0, 1]\ntest_img = np.expand_dims(test_img, axis=0)\ntrue_label = validation_generator.classes[img_index]\n\npredictions = model.predict(test_img)\n\nplt.imshow(test_img[0])\nplt.title(f\"True label: {true_label}, Predicted label: {np.argmax(predictions)}\")\nplt.show()\n", "library": ["numpy", "matplotlib", "tensorflow"], "exlib": ["numpy", "matplotlib", "tensorflow"]}
{"prompt": "\nFile Path : 'data/tensorflow_dataset02'\n### Load the flower dataset from {data/tensorflow_dataset02}. There are 8 subfolders in the folder, each containing a different number of images, one subfolder representing a category, and the name of the subfolder is the label. You need to divide the images for each category into training sets and test sets in a ratio of 4:1. Normalize the size of each image to (200,200,3) and normalize the pixel values of each image to between 0 and 1.\n", "pre_code": "", "code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset02'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Construct a convolutional neural network model. The size of the input should be {(200,200,3)} and the size of the output should be {8} which represents the number of image categories.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset02'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\n", "code": "from tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))  \n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Compile the model. Define Adam as optimizer and SparseCategoricalCrossentropy as loss function. Choose 'accuracy' as the metric to be monitored during training and evaluation. Train the model for {5} epochs.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset02'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))  \n\n", "code": "model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Draw a plot where the accuracy on the training set and the validation set changes with the number of training epochs. Two lines are displayed on one plot with label 'accuracy' and 'val_accuracy'. Set the x-label as 'Epoch' and y-label as 'Accuracy'. Finally display the plot.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset02'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))  \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\n", "code": "import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\n### Evaluate the model. Calculate the result of the test loss plus the test accuracy. Print the result (rounded to two decimal places).\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset02'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))  \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n", "code": "test_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Select a test image (image index = 42) for prediction. Display the image in a plot with the True label and the Predicted label in the title of the plot. The format should be \"True label: {true_label}, Predicted label: {predicted_label}\"\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset02'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(8, activation='softmax'))  \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\ntest_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n\n", "code": "import numpy as np\nfrom tensorflow.keras.preprocessing import image\nimg_index = 2\n\ntest_img_path = validation_generator.filepaths[img_index]\ntest_img = image.load_img(test_img_path, target_size=(200, 200))\ntest_img = image.img_to_array(test_img) / 255.0  # Normalize pixel values to [0, 1]\ntest_img = np.expand_dims(test_img, axis=0)\ntrue_label = validation_generator.classes[img_index]\n\npredictions = model.predict(test_img)\n\nplt.imshow(test_img[0])\nplt.title(f\"True label: {true_label}, Predicted label: {np.argmax(predictions)}\")\nplt.show()\n", "library": ["numpy", "matplotlib", "tensorflow"], "exlib": ["numpy", "matplotlib", "tensorflow"]}
{"prompt": "File Path : 'data/tensorflow_dataset04'\n### Load the flower dataset from {data/tensorflow_dataset04}. There are 10 subfolders in the folder, each containing a different number of images, one subfolder representing a category, and the name of the subfolder is the label. You need to divide the images for each category into training sets and test sets in a ratio of 4:1. Normalize the size of each image to (200,200,3) and normalize the pixel values of each image to between 0 and 1.\n", "pre_code": "", "code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset04'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Construct a convolutional neural network model. The size of the input should be {(200,200,3)} and the size of the output should be {10} which represents the number of image categories.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset04'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\n", "code": "from tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Compile the model. Define Adam as optimizer and SparseCategoricalCrossentropy as loss function. Choose 'accuracy' as the metric to be monitored during training and evaluation. Train the model for {5} epochs.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset04'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\n", "code": "model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Draw a plot where the accuracy on the training set and the validation set changes with the number of training epochs. Two lines are displayed on one plot with label 'accuracy' and 'val_accuracy'. Set the x-label as 'Epoch' and y-label as 'Accuracy'. Finally display the plot.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset04'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\n", "code": "import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\n### Evaluate the model. Calculate the result of the test loss plus the test accuracy. Print the result (rounded to two decimal places).\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset04'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n", "code": "test_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Select a test image (image index = 42) for prediction. Display the image in a plot with the True label and the Predicted label in the title of the plot. The format should be \"True label: {true_label}, Predicted label: {predicted_label}\"\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset04'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\ntest_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n\n", "code": "import numpy as np\nfrom tensorflow.keras.preprocessing import image\nimg_index = 2\n\ntest_img_path = validation_generator.filepaths[img_index]\ntest_img = image.load_img(test_img_path, target_size=(200, 200))\ntest_img = image.img_to_array(test_img) / 255.0  # Normalize pixel values to [0, 1]\ntest_img = np.expand_dims(test_img, axis=0)\ntrue_label = validation_generator.classes[img_index]\n\npredictions = model.predict(test_img)\n\nplt.imshow(test_img[0])\nplt.title(f\"True label: {true_label}, Predicted label: {np.argmax(predictions)}\")\nplt.show()\n", "library": ["numpy", "matplotlib", "tensorflow"], "exlib": ["numpy", "matplotlib", "tensorflow"]}
{"prompt": "File Path: 'data/opencv_dataset06.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset06.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset06.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset06.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset06.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset06.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset06.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path : 'data/tensorflow_dataset05'\n### Load the flower dataset from {data/tensorflow_dataset05}. There are 10 subfolders in the folder, each containing a different number of images, one subfolder representing a category, and the name of the subfolder is the label. You need to divide the images for each category into training sets and test sets in a ratio of 4:1. Normalize the size of each image to (200,200,3) and normalize the pixel values of each image to between 0 and 1.\n", "pre_code": "", "code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset05'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Construct a convolutional neural network model. The size of the input should be {(200,200,3)} and the size of the output should be {10} which represents the number of image categories.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset05'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\n", "code": "from tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Compile the model. Define Adam as optimizer and SparseCategoricalCrossentropy as loss function. Choose 'accuracy' as the metric to be monitored during training and evaluation. Train the model for {5} epochs.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset05'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\n", "code": "model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Draw a plot where the accuracy on the training set and the validation set changes with the number of training epochs. Two lines are displayed on one plot with label 'accuracy' and 'val_accuracy'. Set the x-label as 'Epoch' and y-label as 'Accuracy'. Finally display the plot.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset05'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\n", "code": "import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\n### Evaluate the model. Calculate the result of the test loss plus the test accuracy. Print the result (rounded to two decimal places).\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset05'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n", "code": "test_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Select a test image (image index = 42) for prediction. Display the image in a plot with the True label and the Predicted label in the title of the plot. The format should be \"True label: {true_label}, Predicted label: {predicted_label}\"\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset05'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\ntest_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n\n", "code": "import numpy as np\nfrom tensorflow.keras.preprocessing import image\nimg_index = 2\n\ntest_img_path = validation_generator.filepaths[img_index]\ntest_img = image.load_img(test_img_path, target_size=(200, 200))\ntest_img = image.img_to_array(test_img) / 255.0  # Normalize pixel values to [0, 1]\ntest_img = np.expand_dims(test_img, axis=0)\ntrue_label = validation_generator.classes[img_index]\n\npredictions = model.predict(test_img)\n\nplt.imshow(test_img[0])\nplt.title(f\"True label: {true_label}, Predicted label: {np.argmax(predictions)}\")\nplt.show()\n", "library": ["numpy", "matplotlib", "tensorflow"], "exlib": ["numpy", "matplotlib", "tensorflow"]}
{"prompt": "File Path: `data/sklearn_dataset01.csv`\n\n### Load the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Split the data into features and target \"Species\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Species'], axis=1)\ny = df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### First perform dimensionality reduction using \"PCA\" on the feature data ，then display different categories of data in three dimensions.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Species'], axis=1)\ny = df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Iris Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Establish a MLP classifaction model, using optimizer \"Adam\" and activation function \"relu\". Then conduct model training for 200 epochs.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Species'], axis=1)\ny = df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Iris Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\n", "code": "from sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Draw a loss curve with epoch as the horizontal axis and loss as the vertical axis.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Species'], axis=1)\ny = df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Iris Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\n", "code": "train_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\nMake predictions on the test set, give the confusion matrix, and count the prediction accuracy, precision, recall, and F1 score of each category. Finally, display the accuracy of the model on the test set (rounded to two decimal places).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Species'], axis=1)\ny = df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Iris Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\nVisualize predictive data with 3D plots.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset01.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['Species'], axis=1)\ny = df['Species']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Iris Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n\n", "code": "predictions = model.predict(X_test)\nX_pca_test = pca.transform(X_test)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=LabelEncoder().fit_transform(predictions))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "File Path: 'data/opencv_dataset07.jpg'. \n### Load the image from the file path. Display the original RGB image.\n", "pre_code": "", "code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset07.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Resize the image to (500,560). Convert the image to grayscale and display the converted image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset07.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\n", "code": "resized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Set the Guass kernel size as (3,3) and Guass sigma as 0, apply Guassian blur to the grayscale image. Set the target size as (500,600) and then perform Linear interpolation on the blurred image. Calculate the mean value of the resultant image pixels. Print the result with two decimal places.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset07.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\n", "code": "import numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "\n\n### Applying histogram equalization to the blurred grayscale image to enhance its contrast, followed by displaying the resultant image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset07.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\n", "code": "equalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect edges using the Canny edge detector with canny min-val=80 and canny max-val=200 on the image processed in the previous step. Display the edged image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset07.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\n", "code": "canny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n", "library": ["opencv", "matplotlib"], "exlib": ["opencv", "matplotlib"]}
{"prompt": "\n\n### Detect corners using the Shi-Tomas corner detector with max-corners=50, min-distance=0.5 and blocksize=10, mark the corners with circles on the image. Note that the coordinates of corner points must be a tuple of integers. The radius and thickness of the circle are 5 and 1. Show the marked image.\n", "pre_code": "import cv2\nimport matplotlib.pyplot as plt\nfile_path = 'data/opencv_dataset07.jpg'\nimage = cv2.imread(file_path)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.show()\n\nresized_image = cv2.resize(image, (500, 560))\ngray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\nplt.imshow(gray_image, cmap='gray')\nplt.show()\n\nimport numpy as np\ngaussian_kernel_size = (3, 3)\ngaussian_sigma = 0\nblurred_image = cv2.GaussianBlur(gray_image, gaussian_kernel_size, gaussian_sigma)\ntarget_size = (500, 600)\nresized_image = cv2.resize(blurred_image, target_size, interpolation=cv2.INTER_LINEAR)\nmean_value = np.mean(resized_image)\nround(mean_value,2)\n\nequalized_image = cv2.equalizeHist(blurred_image)\nplt.imshow(equalized_image, cmap='gray')\nplt.show()\n\ncanny_min_val = 80\ncanny_max_val = 200\nedges = cv2.Canny(equalized_image, canny_min_val, canny_max_val)\nplt.imshow(edges, cmap='gray')\nplt.show()\n\n", "code": "max_corners = 50\nmin_distance = 0.5\nblock_size = 10\ncorners = cv2.goodFeaturesToTrack(equalized_image, max_corners, 0.01, min_distance, blockSize=block_size)\ncorners = np.intp(corners)\n\nmarked_image = resized_image.copy()\nfor corner in corners:\n    x, y = corner.ravel()\n    cv2.circle(marked_image, (x, y), 5, (0, 0, 255), 1)  #circle with radius 5 and thickness 1\n\nplt.imshow(cv2.cvtColor(marked_image, cv2.COLOR_BGR2RGB))\nplt.show()\n", "library": ["opencv", "numpy"], "exlib": ["opencv", "numpy"]}
{"prompt": "File Path : 'data/tensorflow_dataset03'\n### Load the flower dataset from {data/tensorflow_dataset03}. There are 12 subfolders in the folder, each containing a different number of images, one subfolder representing a category, and the name of the subfolder is the label. You need to divide the images for each category into training sets and test sets in a ratio of 4:1. Normalize the size of each image to (200,200,3) and normalize the pixel values of each image to between 0 and 1.\n", "pre_code": "", "code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset03'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Construct a convolutional neural network model. The size of the input should be {(200,200,3)} and the size of the output should be {12} which represents the number of image categories.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset03'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\n", "code": "from tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(12, activation='softmax')) \n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Compile the model. Define Adam as optimizer and SparseCategoricalCrossentropy as loss function. Choose 'accuracy' as the metric to be monitored during training and evaluation. Train the model for {5} epochs.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset03'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(12, activation='softmax')) \n\n", "code": "model.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Draw a plot where the accuracy on the training set and the validation set changes with the number of training epochs. Two lines are displayed on one plot with label 'accuracy' and 'val_accuracy'. Set the x-label as 'Epoch' and y-label as 'Accuracy'. Finally display the plot.\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset03'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(12, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\n", "code": "import matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\n### Evaluate the model. Calculate the result of the test loss plus the test accuracy. Print the result (rounded to two decimal places).\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset03'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(12, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\n", "code": "test_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n", "library": ["tensorflow"], "exlib": ["tensorflow"]}
{"prompt": "\n\n### Select a test image (image index = 42) for prediction. Display the image in a plot with the True label and the Predicted label in the title of the plot. The format should be \"True label: {true_label}, Predicted label: {predicted_label}\"\n", "pre_code": "import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\ndata_dir = 'data/tensorflow_dataset03'\ntrain_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\nbatch_size = 32\ntarget_size = (200, 200)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training'\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=target_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation'\n)\n\nfrom tensorflow.keras import layers, models\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(12, activation='softmax')) \n\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nhistory = model.fit(train_generator, epochs=5, validation_data=validation_generator)\n\nimport matplotlib.pyplot as plt\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.show()\n\ntest_loss, test_acc = model.evaluate(validation_generator)\nresult = test_loss + test_acc\nround(result,2)\n\n", "code": "import numpy as np\nfrom tensorflow.keras.preprocessing import image\nimg_index = 2\n\ntest_img_path = validation_generator.filepaths[img_index]\ntest_img = image.load_img(test_img_path, target_size=(200, 200))\ntest_img = image.img_to_array(test_img) / 255.0  # Normalize pixel values to [0, 1]\ntest_img = np.expand_dims(test_img, axis=0)\ntrue_label = validation_generator.classes[img_index]\n\npredictions = model.predict(test_img)\n\nplt.imshow(test_img[0])\nplt.title(f\"True label: {true_label}, Predicted label: {np.argmax(predictions)}\")\nplt.show()\n", "library": ["numpy", "matplotlib", "tensorflow"], "exlib": ["numpy", "matplotlib", "tensorflow"]}
{"prompt": "File Path: `data/sklearn_dataset03.csv`\n\n### Load the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Split the data into features and target \"output\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### First perform dimensionality reduction using \"PCA\" on the feature data ，then display different categories of data in three dimensions.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Heart Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Establish a MLP classifaction model, using optimizer \"Adam\" and activation function \"relu\". Then conduct model training for 200 epochs.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Heart Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\n", "code": "from sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Draw a loss curve with epoch as the horizontal axis and loss as the vertical axis.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Heart Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\n", "code": "train_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Make predictions on the test set, give the confusion matrix, and count the prediction accuracy, precision, recall, and F1 score of each category. Finally, display the accuracy of the model on the test set (rounded to two decimal places).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Heart Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Visualize predictive data with 3D plots.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset03.csv\")\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['output'], axis=1)\ny = df['output']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of Heart Dataset\"\n\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n\n", "code": "predictions = model.predict(X_test)\nX_pca_test = pca.transform(X_test)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=LabelEncoder().fit_transform(predictions))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "File Path: `data/sklearn_dataset05.csv`\n\n### Load the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Split the data into features and target \"condition\", conduct label encoder towards y label and any other non-numeric columns, then into training and testing sets with test size being 0.2.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n\n", "code": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['condition'], axis=1)\ny = df['condition']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Create a SimpleImputer object, populate NaN values in the dataframe with the mean, and fit transformations on X_train and X_test. First perform dimensionality reduction using \"PCA\" on the feature data ，then display different categories of data in three dimensions.\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['condition'], axis=1)\ny = df['condition']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n", "code": "from sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of PittsburghTrees Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Establish a MLP classifaction model, using optimizer \"Adam\" and activation function \"relu\". Then conduct model training for 200 epochs.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['condition'], axis=1)\ny = df['condition']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of PittsburghTrees Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\n", "code": "from sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Draw a loss curve with epoch as the horizontal axis and loss as the vertical axis.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['condition'], axis=1)\ny = df['condition']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of PittsburghTrees Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\n", "code": "train_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "\n\n### Make predictions on the test set, give the confusion matrix, and count the prediction accuracy, precision, recall, and F1 score of each category. Finally, display the accuracy of the model on the test set (rounded to two decimal places).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['condition'], axis=1)\ny = df['condition']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of PittsburghTrees Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\n", "code": "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n", "library": ["sklearn"], "exlib": ["sklearn"]}
{"prompt": "\n\n### Visualize predictive data with 3D plots.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/sklearn_dataset05.csv\", encoding='gbk')\ndf.columns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.drop(columns=['condition'], axis=1)\ny = df['condition']\ny = LabelEncoder().fit_transform(y)\nX = pd.get_dummies(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nimport matplotlib.pyplot as plt\n\nDimensionality_Reduction = \"PCA\"\nPlot_Title = \"PCA of PittsburghTrees Dataset\"\n\nimputer = SimpleImputer(strategy='mean')\nX_train = imputer.fit_transform(X_train)\nX_test = imputer.fit_transform(X_test)\npca = PCA(n_components=3)\nX_pca = pca.fit_transform(X_train)\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=LabelEncoder().fit_transform(y_train))\nax.set_title(Plot_Title)\nplt.show()\n\nfrom sklearn.neural_network import MLPClassifier\n\nModel_Name = \"MLPClassifier\"\nOptimizer = \"adam\"\nActivation_Function = \"relu\"\n\nmodel = MLPClassifier(hidden_layer_sizes=(100,), max_iter=200, activation=Activation_Function, solver=Optimizer, random_state=42)\nmodel.fit(X_train, y_train)\n\ntrain_accuracy = model.score(X_train, y_train)\ntest_accuracy = model.score(X_test, y_test)\n\nplt.figure(figsize=(10,6))\nplt.plot(model.loss_curve_)\nplt.title(\"Loss Curve\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\ny_pred = model.predict(X_test)\ny_true = y_test\n\nconfusion = confusion_matrix(y_true, y_pred)\naccuracy = accuracy_score(y_true, y_pred)\nprecision = precision_score(y_true, y_pred, average=None)\nrecall = recall_score(y_true, y_pred, average=None)\nf1 = f1_score(y_true, y_pred, average=None)\n\nround(accuracy, 2)\n\n", "code": "predictions = model.predict(X_test)\nX_pca_test = pca.transform(X_test)\n\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X_pca_test[:, 0], X_pca_test[:, 1], X_pca_test[:, 2], c=LabelEncoder().fit_transform(predictions))\nax.set_title(Plot_Title)\nplt.show()\n", "library": ["sklearn", "matplotlib"], "exlib": ["sklearn", "matplotlib"]}
{"prompt": "File Path: `data/pandas_dataset01.csv`\n\nLoad the dataset from the file path into a pandas DataFrame using sep=\";\". Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset01.csv\", sep=\";\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset01.csv\", sep=\";\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in fixed acidity with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset01.csv\", sep=\";\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['fixed acidity'].fillna(df['fixed acidity'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of alcohol with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset01.csv\", sep=\";\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['fixed acidity'].fillna(df['fixed acidity'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['alcohol']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\n\nHow many samples have at least two outliers in \"volatile acidity\"; \"citric acid\"; \"residual sugar\" independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset01.csv\", sep=\";\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['fixed acidity'].fillna(df['fixed acidity'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['alcohol']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = [\"volatile acidity\", \"citric acid\", \"residual sugar\"]\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by \"quality\" and calculate the average of each numerical column. Find out how many attributes have a strict positive correlation with \"quality\".(Higher quality, higher value)\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset01.csv\", sep=\";\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['fixed acidity'].fillna(df['fixed acidity'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['alcohol']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = [\"volatile acidity\", \"citric acid\", \"residual sugar\"]\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ngrouped_df = df_num.groupby(\"quality\").agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() > 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path: `data/pandas_dataset03.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset03.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset03.csv\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in SepalLengthCm with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset03.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['SepalLengthCm'].fillna(df['SepalLengthCm'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of SepalWidthCm with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset03.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['SepalLengthCm'].fillna(df['SepalLengthCm'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['SepalWidthCm']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in SepalWidthCm, PetalLengthCm, PetalWidthCm independently. Set outliner zscore greater than 1.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset03.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['SepalLengthCm'].fillna(df['SepalLengthCm'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['SepalWidthCm']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 1)[0]\n\ncolumn_names = ['SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nSelect all Iris-setosa samples, then group and aggregate data by PetalWidthCm and calculate the average of each numerical column. Find out how many attributes have a strict negative correlation with PetalWidthCm in this species.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset03.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['SepalLengthCm'].fillna(df['SepalLengthCm'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['SepalWidthCm']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 1)[0]\n\ncolumn_names = ['SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf = df[df['Species'] == 'Iris-setosa']\ndf_num = df.select_dtypes(include=['float64', 'int64'])\n\ngrouped_df = df_num.groupby('PetalWidthCm').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() < 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path: `data/pandas_dataset02.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset02.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset02.csv\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in mpg with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset02.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['mpg'].fillna(df['mpg'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of weight with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset02.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['mpg'].fillna(df['mpg'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['weight']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in displacement, weight independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset02.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['mpg'].fillna(df['mpg'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['weight']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['displacement', 'weight']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by horsepower and calculate the average of each numerical column. Find out how many attributes have a strict negative correlation with horsepower.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset02.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['mpg'].fillna(df['mpg'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['weight']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['displacement', 'weight']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ngrouped_df = df_num.groupby('horsepower').agg(['mean'])\n\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() < 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path : 'data/scipy_dataset06.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset06.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 4th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset06.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 4th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset06.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 4th column and the 12th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset06.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_12 = data.iloc[:, 11]\nstatistic, p_value = stats.ttest_ind(column_4, column_12)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 4th column and the 13th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset06.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_12 = data.iloc[:, 11]\nstatistic, p_value = stats.ttest_ind(column_4, column_12)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_13 = data.iloc[:, 12]\nresult = linregress(column_4, column_13)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 4th,12th,13th and 14th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset06.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['AGE', 'INCIDENTZONE',\t'INCIDENTTRACT',\t'COUNCIL_DISTRICT']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_12 = data.iloc[:, 11]\nstatistic, p_value = stats.ttest_ind(column_4, column_12)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_13 = data.iloc[:, 12]\nresult = linregress(column_4, column_13)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [3, 11, 12, 13]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path: `data/pandas_dataset05.xlsx`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset05.xlsx\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset05.xlsx\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in Monthly Income with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset05.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['Monthly Income'].fillna(df['Monthly Income'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of Balance with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset05.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Monthly Income'].fillna(df['Monthly Income'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Balance']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in balance, debt and age independently. Set outliner zscore greater than 1.5.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset05.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Monthly Income'].fillna(df['Monthly Income'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Balance']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 1.5)[0]\n\ncolumn_names = ['Balance', 'Debt', 'Age']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nDrop date related column, then group and aggregate data by credit rating and calculate the average of each column. Find out how many attributes have a strict positive correlation with credit rating. (C is the best credit rating)\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset05.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Monthly Income'].fillna(df['Monthly Income'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Balance']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 1.5)[0]\n\ncolumn_names = ['Balance', 'Debt', 'Age']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf = df.drop('Last Loan Date', axis=1)\ngrouped_df = df.groupby('Credit Rating').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() > 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path : 'data/scipy_dataset10.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset10.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 6th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset10.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 6th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset10.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 6th column and the 7th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset10.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_7 = data.iloc[:, 6]\nstatistic, p_value = stats.ttest_ind(column_6, column_7)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 8th column and the 9th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset10.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_7 = data.iloc[:, 6]\nstatistic, p_value = stats.ttest_ind(column_6, column_7)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_8 = data.iloc[:, 7]\ncolumn_9 = data.iloc[:, 8]\nresult = linregress(column_8, column_9)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 6,7,8,9th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset10.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Malignant Neoplasms',\t'Diabetes Mellitus',\t'Alzheimer Disease',\t'Influenza and Pneumonia']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_7 = data.iloc[:, 6]\nstatistic, p_value = stats.ttest_ind(column_6, column_7)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_8 = data.iloc[:, 7]\ncolumn_9 = data.iloc[:, 8]\nresult = linregress(column_8, column_9)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [5,6,7,8]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path : 'data/scipy_dataset03.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset03.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 5th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset03.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_5 = data.iloc[:,4]\nmean = column_5.mean()\nmedian = column_5.median()\nvariance = column_5.var()\nskewness = column_5.skew()\nkurtosis = column_5.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 5th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset03.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n\nimport numpy as np\nfrom scipy import stats\ncolumn_5 = data.iloc[:,4]\nmean = column_5.mean()\nmedian = column_5.median()\nvariance = column_5.var()\nskewness = column_5.skew()\nkurtosis = column_5.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 5th column and the 6th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset03.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n\nimport numpy as np\nfrom scipy import stats\ncolumn_5 = data.iloc[:,4]\nmean = column_5.mean()\nmedian = column_5.median()\nvariance = column_5.var()\nskewness = column_5.skew()\nkurtosis = column_5.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_5, column_6)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 9th column and the 10th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset03.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n\nimport numpy as np\nfrom scipy import stats\ncolumn_5 = data.iloc[:,4]\nmean = column_5.mean()\nmedian = column_5.median()\nvariance = column_5.var()\nskewness = column_5.skew()\nkurtosis = column_5.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_5, column_6)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_9 = data.iloc[:, 8]\ncolumn_10 = data.iloc[:, 9]\nresult = linregress(column_9, column_10)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 5th,6th,9th and 10th columns into 4 clusters. Please use Kmeans algorithm. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15 (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset03.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['MUNICIPALITY','POLICE_AGCY','TIME_OF_DAY','HOUR_OF_DAY']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n\nimport numpy as np\nfrom scipy import stats\ncolumn_5 = data.iloc[:,4]\nmean = column_5.mean()\nmedian = column_5.median()\nvariance = column_5.var()\nskewness = column_5.skew()\nkurtosis = column_5.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_5, column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_9 = data.iloc[:, 8]\ncolumn_10 = data.iloc[:, 9]\nresult = linregress(column_9, column_10)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [4, 5, 8, 9]]\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\nFile Path : 'data/scipy_dataset09.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Choose 'gbk' as the encoding mode. Remove rows with non-numeric data in columns ['height',\t'width',\t'growth_space_length',\t'growth_space_width']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset09.csv'\ndata = pd.read_csv(csv_filename, header = 0, encoding='gbk')\ncolumns_to_convert = ['height',\t'width',\t'growth_space_length',\t'growth_space_width']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 7th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset09.csv'\ndata = pd.read_csv(csv_filename, header = 0, encoding='gbk')\ncolumns_to_convert = ['height',\t'width',\t'growth_space_length',\t'growth_space_width']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_7 = data.iloc[:,6]\nmean = column_7.mean()\nmedian = column_7.median()\nvariance = column_7.var()\nskewness = column_7.skew()\nkurtosis = column_7.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 7th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset09.csv'\ndata = pd.read_csv(csv_filename, header = 0, encoding='gbk')\ncolumns_to_convert = ['height',\t'width',\t'growth_space_length',\t'growth_space_width']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_7 = data.iloc[:,6]\nmean = column_7.mean()\nmedian = column_7.median()\nvariance = column_7.var()\nskewness = column_7.skew()\nkurtosis = column_7.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_7)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 7th column and the 8th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset09.csv'\ndata = pd.read_csv(csv_filename, header = 0, encoding='gbk')\ncolumns_to_convert = ['height',\t'width',\t'growth_space_length',\t'growth_space_width']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_7 = data.iloc[:,6]\nmean = column_7.mean()\nmedian = column_7.median()\nvariance = column_7.var()\nskewness = column_7.skew()\nkurtosis = column_7.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_7)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_8 = data.iloc[:, 7]\nstatistic, p_value = stats.ttest_ind(column_7, column_8)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 8th column and the 9th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset09.csv'\ndata = pd.read_csv(csv_filename, header = 0, encoding='gbk')\ncolumns_to_convert = ['height',\t'width',\t'growth_space_length',\t'growth_space_width']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_7 = data.iloc[:,6]\nmean = column_7.mean()\nmedian = column_7.median()\nvariance = column_7.var()\nskewness = column_7.skew()\nkurtosis = column_7.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_7)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_8 = data.iloc[:, 7]\nstatistic, p_value = stats.ttest_ind(column_7, column_8)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_8 = data.iloc[:, 7]\ncolumn_9 = data.iloc[:, 8]\nresult = linregress(column_8, column_9)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 7,8,9,10th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset09.csv'\ndata = pd.read_csv(csv_filename, header = 0, encoding='gbk')\ncolumns_to_convert = ['height',\t'width',\t'growth_space_length',\t'growth_space_width']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_7 = data.iloc[:,6]\nmean = column_7.mean()\nmedian = column_7.median()\nvariance = column_7.var()\nskewness = column_7.skew()\nkurtosis = column_7.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_7)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_8 = data.iloc[:, 7]\nstatistic, p_value = stats.ttest_ind(column_7, column_8)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_8 = data.iloc[:, 7]\ncolumn_9 = data.iloc[:, 8]\nresult = linregress(column_8, column_9)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [6,7,8,9]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path: `data/pandas_dataset06.xlsx`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset06.xlsx\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset06.xlsx\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in Exam Score with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset06.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['Exam Score'].fillna(df['Exam Score'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of Attendance with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset06.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Exam Score'].fillna(df['Exam Score'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Attendance']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in Attendance, homework completion and online exam count independently. Set outliner zscore greater than 1.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset06.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Exam Score'].fillna(df['Exam Score'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Attendance']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 1)[0]\n\ncolumn_names = ['Attendance', 'Homework Completion', 'Online Exam Count']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nMapping performance, 0 for average, 1 for good, 2 for excellent, then group and aggregate data by performance and calculate the average of each column. Find out how many attributes have a strict positive correlation with performance.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset06.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Exam Score'].fillna(df['Exam Score'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Attendance']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 1)[0]\n\ncolumn_names = ['Attendance', 'Homework Completion', 'Online Exam Count']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf['Performance'] = df['Performance'].map(\n  {'average': 0, 'good': 1, 'excellent': 2}\n)\ngrouped_df = df.groupby('Performance').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() > 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path: `data/pandas_dataset04.xlsx`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset04.xlsx\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset04.xlsx\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in weight with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset04.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['Weight'].fillna(df['Weight'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of height with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset04.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Weight'].fillna(df['Weight'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Height']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in blood sugar, oxygen saturation and white blood cell count independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset04.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Weight'].fillna(df['Weight'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Height']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['Blood Sugar', 'Oxygen Saturation', 'White Blood Cell Count']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by health status and calculate the average of each columns. Find out how many attributes have a strict negative correlation with weight.(Healthier, lower values)\n\n", "pre_code": "import pandas as pd\ndf = pd.read_excel(\"data/pandas_dataset04.xlsx\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Weight'].fillna(df['Weight'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['Height']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['Blood Sugar', 'Oxygen Saturation', 'White Blood Cell Count']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "grouped_df = df.groupby('Health Status').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() > 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path: `data/pandas_dataset07.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset07.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset07.csv\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in Administrative with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset07.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['Administrative'].fillna(df['Administrative'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of ProductRelated_Duration with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset07.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Administrative'].fillna(df['Administrative'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['ProductRelated_Duration']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in BounceRates, ExitRates, PageValues independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset07.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Administrative'].fillna(df['Administrative'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['ProductRelated_Duration']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['BounceRates', 'ExitRates', 'PageValues']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by TrafficType and calculate the average of each numerical column. Find out how many attributes have a strict negative correlation with TrafficType.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset07.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Administrative'].fillna(df['Administrative'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['ProductRelated_Duration']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['BounceRates', 'ExitRates', 'PageValues']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ngrouped_df = df_num.groupby('TrafficType').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() < 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path: `data/pandas_dataset09.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset09.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset09.csv\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in work_year with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset09.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['work_year'].fillna(df['work_year'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of salary with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset09.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['work_year'].fillna(df['work_year'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['salary']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in salary, salary_in_usd and remote_ratio independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset09.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['work_year'].fillna(df['work_year'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['salary']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['salary', 'salary_in_usd', 'remote_ratio']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by work_year and calculate the average of each numerical column. Find out how many attributes have a strict positive correlation with work_year.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset09.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['work_year'].fillna(df['work_year'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['salary']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['salary', 'salary_in_usd', 'remote_ratio']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ngrouped_df = df_num.groupby('work_year').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() > 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path : 'data/scipy_dataset04.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset04.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 4th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset04.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 4th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset04.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n\n\n### Use the t-test to compare the difference between the 4th column and the 6th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset04.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_4, column_6)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 4th column and the 5th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset04.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_4, column_6)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_5 = data.iloc[:, 4]\nresult = linregress(column_4, column_5)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 4th,5th,6th and 7th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset04.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TPAN2',\t'TWAD',\t'TWAN',\t'TWAN2']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_4, column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_5 = data.iloc[:, 4]\nresult = linregress(column_4, column_5)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [3, 4, 5, 6]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path : 'data/scipy_dataset08.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset08.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 6th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset08.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 6th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset08.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 6th column and the 7th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset08.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_7 = data.iloc[:, 6]\nstatistic, p_value = stats.ttest_ind(column_6, column_7)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 10th column and the 11th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset08.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_7 = data.iloc[:, 6]\nstatistic, p_value = stats.ttest_ind(column_6, column_7)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_10 = data.iloc[:, 9]\ncolumn_11 = data.iloc[:, 10]\nresult = linregress(column_10, column_11)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 6,7,10,11th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset08.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['TotalPopEst2015_19ACS',\t'Age0to17PopEst2015_19ACS',\t'WellChildVisitsInPastYearAge0to17','WellChildVisitsInPastYearPer100PrimaryCarePatients']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_6 = data.iloc[:,5]\nmean = column_6.mean()\nmedian = column_6.median()\nvariance = column_6.var()\nskewness = column_6.skew()\nkurtosis = column_6.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_7 = data.iloc[:, 6]\nstatistic, p_value = stats.ttest_ind(column_6, column_7)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_10 = data.iloc[:, 9]\ncolumn_11 = data.iloc[:, 10]\nresult = linregress(column_10, column_11)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [5,6,9,10]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path: `data/pandas_dataset08.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset08.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset08.csv\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in iforderpv_24h with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset08.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['iforderpv_24h'].fillna(df['iforderpv_24h'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of consuming_capacity with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset08.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['iforderpv_24h'].fillna(df['iforderpv_24h'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['consuming_capacity']), color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in price_sensitive, hoteluv, businessrate_pre independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset08.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['iforderpv_24h'].fillna(df['iforderpv_24h'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['consuming_capacity']), color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['price_sensitive', 'hoteluv', 'businessrate_pre']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by label and calculate the average of each numerical column. Find out how many attributes have a strict negative correlation with label.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset08.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['iforderpv_24h'].fillna(df['iforderpv_24h'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\nsns.displot(np.sqrt(df['consuming_capacity']), color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['price_sensitive', 'hoteluv', 'businessrate_pre']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ngrouped_df = df_num.groupby('label').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() < 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path : 'data/scipy_dataset05.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset05.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 4th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset05.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 4th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset05.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 4th column and the 6th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset05.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_4, column_6)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 6th column and the 7th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset05.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_4, column_6)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_6 = data.iloc[:, 5]\ncolumn_7 = data.iloc[:, 6]\nresult = linregress(column_6, column_7)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 4th,6th,7th and 8th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset05.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['Street Number', 'ZIP Code',\t'Lat',\t'Lon']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:,3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_4)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_6 = data.iloc[:, 5]\nstatistic, p_value = stats.ttest_ind(column_4, column_6)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_6 = data.iloc[:, 5]\ncolumn_7 = data.iloc[:, 6]\nresult = linregress(column_6, column_7)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [3, 5, 6, 7]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\nFile Path : 'data/scipy_dataset07.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Remove rows with non-numeric data in columns ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']. Get the information of the first 5 rows of data. \n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset07.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 9th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset07.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_9 = data.iloc[:,8]\nmean = column_9.mean()\nmedian = column_9.median()\nvariance = column_9.var()\nskewness = column_9.skew()\nkurtosis = column_9.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n\n### Compute the Shapiro-Wilk test for the 9th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset07.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_9 = data.iloc[:,8]\nmean = column_9.mean()\nmedian = column_9.median()\nvariance = column_9.var()\nskewness = column_9.skew()\nkurtosis = column_9.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_9)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 8th column and the 9th column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset07.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_9 = data.iloc[:,8]\nmean = column_9.mean()\nmedian = column_9.median()\nvariance = column_9.var()\nskewness = column_9.skew()\nkurtosis = column_9.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_9)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_8 = data.iloc[:, 7]\nstatistic, p_value = stats.ttest_ind(column_8, column_9)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 10th column and the 11th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset07.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_9 = data.iloc[:,8]\nmean = column_9.mean()\nmedian = column_9.median()\nvariance = column_9.var()\nskewness = column_9.skew()\nkurtosis = column_9.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_9)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_8 = data.iloc[:, 7]\nstatistic, p_value = stats.ttest_ind(column_8, column_9)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_10 = data.iloc[:, 9]\ncolumn_11 = data.iloc[:, 10]\nresult = linregress(column_10, column_11)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the 8,9,10,11th columns into 4 clusters. Please use Kmeans algorithm. Remember to convert the numeric type of column to float. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset07.csv'\ndata = pd.read_csv(csv_filename, header = 0)\ncolumns_to_convert = ['water_use_k_gals',\t'wastewater_use_k_gals',\t'total_energy_cost',\t'electricity_cost']\ndata[columns_to_convert] = data[columns_to_convert].apply(pd.to_numeric, errors='coerce')\ndata = data.dropna(subset=columns_to_convert)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_9 = data.iloc[:,8]\nmean = column_9.mean()\nmedian = column_9.median()\nvariance = column_9.var()\nskewness = column_9.skew()\nkurtosis = column_9.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\nstatistic, p_value = stats.shapiro(column_9)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_8 = data.iloc[:, 7]\nstatistic, p_value = stats.ttest_ind(column_8, column_9)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_10 = data.iloc[:, 9]\ncolumn_11 = data.iloc[:, 10]\nresult = linregress(column_10, column_11)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data.iloc[:, [7, 8, 9, 10]].astype(float)\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path : 'data/scipy_dataset02.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe. Get the information of the first 5 rows of data.\n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset02.csv'\ndata = pd.read_csv(csv_filename, header = None)\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 4th column . Add all the results together and print it (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset02.csv'\ndata = pd.read_csv(csv_filename, header = None)\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:, 3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 5th column to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset02.csv'\ndata = pd.read_csv(csv_filename, header = None)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:, 3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\ncolumn_5 = data.iloc[:, 4]\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 2nd column and the 3rd column. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset02.csv'\ndata = pd.read_csv(csv_filename, header = None)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:, 3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\ncolumn_5 = data.iloc[:, 4]\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_2 = data.iloc[:, 1]\ncolumn_3 = data.iloc[:, 2]\nstatistic, p_value = stats.ttest_ind(column_2, column_3)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 6th column and the 7th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset02.csv'\ndata = pd.read_csv(csv_filename, header = None)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:, 3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\ncolumn_5 = data.iloc[:, 4]\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_2 = data.iloc[:, 1]\ncolumn_3 = data.iloc[:, 2]\nstatistic, p_value = stats.ttest_ind(column_2, column_3)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\ncolumn_6 = data.iloc[:, 5]\ncolumn_7 = data.iloc[:, 6]\nresult = linregress(column_6, column_7)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for all the columns into 4 clusters. Please use Kmeans algorithm. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset02.csv'\ndata = pd.read_csv(csv_filename, header = None)\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_4 = data.iloc[:, 3]\nmean = column_4.mean()\nmedian = column_4.median()\nvariance = column_4.var()\nskewness = column_4.skew()\nkurtosis = column_4.kurtosis()\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\ncolumn_5 = data.iloc[:, 4]\nstatistic, p_value = stats.shapiro(column_5)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_2 = data.iloc[:, 1]\ncolumn_3 = data.iloc[:, 2]\nstatistic, p_value = stats.ttest_ind(column_2, column_3)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\ncolumn_6 = data.iloc[:, 5]\ncolumn_7 = data.iloc[:, 6]\nresult = linregress(column_6, column_7)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data[[1,2,3,4,5,6,7,8]]\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "File Path: `data/pandas_dataset10.csv`\n\nLoad the dataset from the file path into a pandas DataFrame. Display the column names.\n\n", "pre_code": "", "code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset10.csv\")\ndf.columns\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nCalculate and display the number of all missing values in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset10.csv\")\ndf.columns\n\n", "code": "sum(df.isnull().sum())\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nReplace missing values in Culmen Length (mm) with the median value of the same column and then remove any remaining rows with missing values. Show the remaining total number of examples in this dataset.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset10.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\n", "code": "df['Culmen Length (mm)'].fillna(df['Culmen Length (mm)'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nGive the data distribution of the square root of Culmen Depth (mm) with red theme.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset10.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Culmen Length (mm)'].fillna(df['Culmen Length (mm)'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\n", "code": "import seaborn as sns\nimport numpy as np\ndf['Culmen Depth (mm)'] = np.sqrt(df['Culmen Depth (mm)'])\nsns.displot(df['Culmen Depth (mm)'], color='red')\n", "library": ["seaborn", "numpy"], "exlib": ["seaborn", "numpy"]}
{"prompt": "\n\nHow many samples have at least two outliers in Culmen Depth (mm), Flipper Length (mm), Body Mass (g) independently. Set outliner zscore greater than 2.\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset10.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Culmen Length (mm)'].fillna(df['Culmen Length (mm)'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\ndf['Culmen Depth (mm)'] = np.sqrt(df['Culmen Depth (mm)'])\nsns.displot(df['Culmen Depth (mm)'], color='red')\n\n", "code": "from scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\nGroup and aggregate data by Body Mass (g) and calculate the average of each numerical column. Find out how many attributes have a strict negative correlation with Body Mass (g).\n\n", "pre_code": "import pandas as pd\ndf = pd.read_csv(\"data/pandas_dataset10.csv\")\ndf.columns\n\nsum(df.isnull().sum())\n\ndf['Culmen Length (mm)'].fillna(df['Culmen Length (mm)'].median(), inplace=True)\ndf.dropna(inplace=True)\nlen(df)\n\nimport seaborn as sns\nimport numpy as np\ndf['Culmen Depth (mm)'] = np.sqrt(df['Culmen Depth (mm)'])\nsns.displot(df['Culmen Depth (mm)'], color='red')\n\nfrom scipy.stats import zscore\nfrom collections import defaultdict\n\ndef get_indices(df, column):\n    z_scores = zscore(df[column])\n    abs_z_scores = np.abs(z_scores)\n    return np.where(abs_z_scores > 2)[0]\n\ncolumn_names = ['Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\n# 创建一个字典来计算每个元素出现的次数\ncounts = defaultdict(int)\nfor column in column_names:\n    indices = get_indices(df, column)\n    for item in indices:\n        counts[item] += 1\n\n# 找出出现两次及以上的元素\nduplicates = {item for item, count in counts.items() if count >= 2}\nlen(duplicates)\n\n", "code": "# Select only the numeric columns as df_num\ndf_num = df.select_dtypes(include=['float64', 'int64'])\ngrouped_df = df_num.groupby('Body Mass (g)').agg(['mean'])\ncount = 0\nfor col in grouped_df.columns:\n    count += (grouped_df[col].diff().dropna() < 0).all()\ncount\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "File Path : 'data/scipy_dataset01.csv'.\n\n### Load the dataset from the file path and turn it into a pandas dataframe.  The index of the column should be named after [SepalLength, SepalWidth, PetalLength, PetalWidth, Class] in turn. Get the information of the first 5 rows of data.\n", "pre_code": "", "code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset01.csv'\ndata = pd.read_csv(csv_filename, header=None, names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\ndata.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\n### Compute the mean, median, variance, skewness, and kurtosis of the 2nd column 'SepalWidth'. Add all the results together and print it(rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset01.csv'\ndata = pd.read_csv(csv_filename, header=None, names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\ndata.head()\n\n", "code": "import numpy as np\nfrom scipy import stats\ncolumn_2 = data['SepalWidth']\nmean = np.mean(column_2)\nmedian = np.median(column_2)\nvariance = np.var(column_2)\nskewness = stats.skew(column_2)\nkurtosis = stats.kurtosis(column_2)\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n", "library": ["scipy", "numpy"], "exlib": ["scipy", "numpy"]}
{"prompt": "\n\n### Compute the Shapiro-Wilk test for the 3rd column 'PetalLength' to check for normality. Calculate the result of the Shapiro-Wilk test statistic plus the p value, print the result(rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset01.csv'\ndata = pd.read_csv(csv_filename, header=None, names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_2 = data['SepalWidth']\nmean = np.mean(column_2)\nmedian = np.median(column_2)\nvariance = np.var(column_2)\nskewness = stats.skew(column_2)\nkurtosis = stats.kurtosis(column_2)\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\n", "code": "from scipy.stats import shapiro\ncolumn_3 = data[\"PetalLength\"]\nstatistic, p_value = shapiro(column_3)\nresult = statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Use the t-test to compare the difference between the 3rd column and the 4th column 'PetalWidth'. Calculate the result of the t-statistic plus p-value, print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset01.csv'\ndata = pd.read_csv(csv_filename, header=None, names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_2 = data['SepalWidth']\nmean = np.mean(column_2)\nmedian = np.median(column_2)\nvariance = np.var(column_2)\nskewness = stats.skew(column_2)\nkurtosis = stats.kurtosis(column_2)\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\ncolumn_3 = data[\"PetalLength\"]\nstatistic, p_value = shapiro(column_3)\nresult = statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import ttest_ind\ncolumn_4 = data[\"PetalWidth\"]\nt_statistic, p_value = ttest_ind(column_3, column_4)\nresult = t_statistic + p_value\nround(result,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Perform linear regression analysis on the 3th column and the 4th column. Add the slope, intercept, r_value, p_value, and standard error together and print the result (rounded to two decimal places).\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset01.csv'\ndata = pd.read_csv(csv_filename, header=None, names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_2 = data['SepalWidth']\nmean = np.mean(column_2)\nmedian = np.median(column_2)\nvariance = np.var(column_2)\nskewness = stats.skew(column_2)\nkurtosis = stats.kurtosis(column_2)\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\ncolumn_3 = data[\"PetalLength\"]\nstatistic, p_value = shapiro(column_3)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_4 = data[\"PetalWidth\"]\nt_statistic, p_value = ttest_ind(column_3, column_4)\nresult = t_statistic + p_value\nround(result,2)\n\n", "code": "from scipy.stats import linregress\nresult = linregress(column_3, column_4)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "\n\n### Classify all rows for the four columns \"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\" into 4 clusters. Please use Kmeans algorithm. Add a column named \"Cluster\" to the dataframe and fill the column with the class to which each row belongs. Print the cluster of the data with index 15.\n", "pre_code": "import pandas as pd\ncsv_filename = 'data/scipy_dataset01.csv'\ndata = pd.read_csv(csv_filename, header=None, names=['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Class'])\ndata.head()\n\nimport numpy as np\nfrom scipy import stats\ncolumn_2 = data['SepalWidth']\nmean = np.mean(column_2)\nmedian = np.median(column_2)\nvariance = np.var(column_2)\nskewness = stats.skew(column_2)\nkurtosis = stats.kurtosis(column_2)\nvalue = sum([mean, median, variance, skewness, kurtosis])\nround(value,2)\n\nfrom scipy.stats import shapiro\ncolumn_3 = data[\"PetalLength\"]\nstatistic, p_value = shapiro(column_3)\nresult = statistic + p_value\nround(result,2)\n\nfrom scipy.stats import ttest_ind\ncolumn_4 = data[\"PetalWidth\"]\nt_statistic, p_value = ttest_ind(column_3, column_4)\nresult = t_statistic + p_value\nround(result,2)\n\nfrom scipy.stats import linregress\nresult = linregress(column_3, column_4)\nvalues = sum([result.slope, result.intercept, result.rvalue, result.pvalue, result.stderr])\nround(values,2)\n\n", "code": "from scipy.cluster.vq import kmeans, vq\nnum_clusters = 4\ndata_for_clustering = data[[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]]\ncentroids, _ = kmeans(data_for_clustering.values, num_clusters)\ncluster_labels, _ = vq(data_for_clustering.values, centroids)\ndata[\"Cluster\"] = cluster_labels\nprint(data.iloc[14][\"Cluster\"])\n", "library": ["scipy"], "exlib": ["scipy"]}
{"prompt": "Text Content: \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Natural language processing (NLP) is a field of computer science, artificial intelligence, and linguistics concerned with the interactions between computers and human (natural) languages.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Deep learning is a subset of machine learning in artificial intelligence (AI) that has networks capable of learning unsupervised from data that is unstructured or unlabeled. Also known as deep neural learning or deep neural network. Deep learning is inspired by the structure and function of the brain, specifically the interconnecting of many neurons. It interprets the data with a logical structure, which is a machine perception. The primary aim is to move the machine closer to some sort of artificial intelligence.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Big data refers to the vast volume of structured and unstructured data that is so large it is difficult to process using traditional database and software techniques. In most enterprise scenarios, the volume of data is too big, moves too fast, or exceeds current processing capacity. Big data has the potential to help companies improve operations and make faster, more intelligent decisions.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Artificial intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and act like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving. The ideal characteristic of artificial intelligence is its ability to rationalize and take actions that have the best chance of achieving a specific goal.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Romeo and Juliet is a tragedy written by William Shakespeare. It is among the most popular plays ever written in the English language. The play revolves around two young star-crossed lovers whose deaths ultimately reconcile their feuding families.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention. Because of new computing technologies, machine learning today is not like machine learning of the past.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The Sun is the star at the center of the Solar System. It is a nearly perfect sphere of hot plasma, with internal convective motion that generates a magnetic field via a dynamo process. It is by far the most important source of energy for life on Earth. Its diameter is about 1.39 million kilometers.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"Blockchain is a system of recording information in a way that makes it difficult or impossible to change, hack, or cheat the system. A blockchain is essentially a digital ledger of transactions that is duplicated and distributed across the entire network of computer systems on the blockchain. Each block in the chain contains a number of transactions, and every time a new transaction occurs on the blockchain, a record of that transaction is added to every participant's ledger.\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"The quick brown fox jumped over the lazy dog. It was an amazing sight to behold. The dog didn't even move!\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "Text Content: \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\n\nLoad the text and convert all words to lowercase. Tokenize the text based on word tokenizer.\n\n", "pre_code": "", "code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nRemove stop words from the text using \"english\". Remove punctuation from the list of tokens. Finally report the number of tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\n", "code": "import nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nImplement stemming for theses tokens. Display the first five tokens in the stemmed_tokens.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\n", "code": "from nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nCalculate frequency distribution of stemmed tokens. Print the token with the most frequency. \n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\n", "code": "from nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nVisualize the cumulative frequency distribution with first ten most frequent tokens using a line chart with figsize=(12, 6).\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\n", "code": "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n", "library": ["matplotlib"], "exlib": ["matplotlib"]}
{"prompt": "\n\nImplement part-of-speech tagging for the tokens after the second step. Only display the sum of number of nouns, verbs and adjectives.\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\n", "code": "from nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nBased on previous pos tagging. Calculate the group(nouns, verbs, adjectives and adverbs) normalized frequency in tokens. Only display the highest frequency(Keep to two decimal places).\n\n\n", "pre_code": "from nltk.tokenize import word_tokenize\nai_text = \"\"\"\nWhen we look to the individuals of the same variety or sub-variety of our older cultivated plants and animals, one of the first points which strikes us, is, that they generally differ much more from each other, than do the individuals of any one species or variety in a state of nature. But nothing is easy to specify cases where an animal or plant in a state of nature presents some slight abnormal structure, or instinct.\n\"\"\"\nlowercase_text = ai_text.lower()\ntokens = word_tokenize(lowercase_text)\n\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nnltk.download('punkt')\nnltk.download('stopwords')\n\nstop_words = set(stopwords.words(\"english\"))\ntokens = [word for word in tokens if word not in stop_words]\ntokens = [word for word in tokens if word not in string.punctuation]\nlen(tokens)\n\nfrom nltk.stem import PorterStemmer\n\nstemmer = PorterStemmer()\nstemmed_tokens = [stemmer.stem(word) for word in tokens]\nprint(stemmed_tokens[:5])\n\nfrom nltk.probability import FreqDist\n\nfreq_dist = FreqDist(stemmed_tokens)\ncommon_tokens = freq_dist.most_common(10)\ncommon_tokens[0]\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 6))\nfreq_dist.plot(10, cumulative=True)\nplt.show()\n\nfrom nltk.tag import pos_tag\nfrom collections import Counter\n\npos_tags = pos_tag(tokens)\n# Counting part-of-speech tags\npos_counts = Counter(tag for word, tag in pos_tags)\n\n# Extracting counts for nouns, verbs, adjectives, and adverbs\nnoun_count = sum(val for key, val in pos_counts.items() if key.startswith('N'))\nverb_count = sum(val for key, val in pos_counts.items() if key.startswith('V'))\nadjective_count = sum(val for key, val in pos_counts.items() if key.startswith('J'))\n\nnoun_count + verb_count + adjective_count\n\n", "code": "adverb_count = sum(val for key, val in pos_counts.items() if key.startswith('R'))\n\nnormalized_nouns = round(noun_count / len(pos_tags), 2)\nnormalized_verbs = round(verb_count / len(pos_tags), 2)\nnormalized_adjectives = round(adjective_count / len(pos_tags), 2)\nnormalized_adverbs = round(adverb_count / len(pos_tags), 2)\n\n# Calculate total number of tokens\nmax([normalized_nouns, normalized_verbs, normalized_adjectives, normalized_adverbs])\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "File Path: 'data/pytorch_dataset04.csv'\n\nLoad the dataset from the file path into a pandas DataFrame using encoding='latin-1'. Display the first 5 rows.\n", "pre_code": "", "code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nPreprocess the data and use labelencoder to labels. Tokenize the text data and convert to lowercase. Remove the punctuations and stopwords from the text data. Finally build a vocabulary of all the tokens and assign an index to the vocabulary. Display the index of the word \"happy\".\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n\n", "code": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['labels'] = LabelEncoder().fit_transform(tweets_df['labels'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['tweets'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nDefine the maximum length of the sequence to 50, and digitize the text to facilitate subsequent text processing.\nDisplay the numericalized representation of the first text.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['labels'] = LabelEncoder().fit_transform(tweets_df['labels'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['tweets'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n", "code": "# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nSplit the dataset into training and testing sets using 0.2 as the test size, then define the train_loader and test_loader. Set batch size as 64.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['labels'] = LabelEncoder().fit_transform(tweets_df['labels'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['tweets'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\n", "code": "import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['labels'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "\n\nDefine a \"SentimentAnalysisLSTM\" model using vocab_size，embedding_dim, hidden_dim, output_dim, n_layers. Put the model to device.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['labels'] = LabelEncoder().fit_transform(tweets_df['labels'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['tweets'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['labels'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n", "code": "import torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nDefine the CrossEntropyLoss function and the Adam optimizer. Train the model for 3 epochs. Display the loss for the last epoch(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['labels'] = LabelEncoder().fit_transform(tweets_df['labels'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['tweets'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['labels'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\n", "code": "import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 2:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nEvaluate the model performance on the test set. Report the model's accuracy(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset04.csv', encoding='latin-1')\n\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['labels'] = LabelEncoder().fit_transform(tweets_df['labels'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['tweets'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['labels'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 2:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n\n", "code": "import numpy as np\n\nmodel.eval()\npredictions, truths = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = model(inputs)\n        predictions.append(outputs.squeeze())\n        truths.append(labels)\n        \npredictions = torch.cat(predictions).cpu().numpy()\ntruths = torch.cat(truths).cpu().numpy()\npredictions = np.round(predictions)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(truths, predictions)\nround(accuracy, 2)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "File Path: 'data/pytorch_dataset03.csv'\n\nLoad the dataset from the file path into a pandas DataFrame using encoding='latin-1'. Display the first 5 rows.\n", "pre_code": "", "code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nPreprocess the data and use labelencoder to sentiment. Tokenize the text data and convert to lowercase. Remove the punctuations and stopwords from the text data. Finally build a vocabulary of all the tokens and assign an index to the vocabulary. Display the index of the word \"happy\".\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n\n", "code": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nDefine the maximum length of the sequence to 50, and digitize the text to facilitate subsequent text processing.\nDisplay the numericalized representation of the first text.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n", "code": "# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nSplit the dataset into training and testing sets using 0.2 as the test size, then define the train_loader and test_loader. Set batch size as 64.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\n", "code": "import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "\n\nDefine a \"SentimentAnalysisLSTM\" model using vocab_size，embedding_dim, hidden_dim, output_dim, n_layers. Put the model to device.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n", "code": "import torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nDefine the CrossEntropyLoss function and the Adam optimizer. Train the model for 3 epochs. Display the loss for the last epoch(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\n", "code": "import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 2:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nEvaluate the model performance on the test set. Report the model's accuracy(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset03.csv', encoding='latin-1')\n# convert text to string\ntweets_df['text'] = tweets_df['text'].astype(str)\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 50  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 2:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n\n", "code": "import numpy as np\n\nmodel.eval()\npredictions, truths = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = model(inputs)\n        predictions.append(outputs.squeeze())\n        truths.append(labels)\n        \npredictions = torch.cat(predictions).cpu().numpy()\ntruths = torch.cat(truths).cpu().numpy()\npredictions = np.round(predictions)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(truths, predictions)\nround(accuracy, 2)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "File Path: 'data/pytorch_dataset02.csv'\n\nLoad the dataset from the file path into a pandas DataFrame using encoding='latin-1'. Display the first 5 rows.\n", "pre_code": "", "code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nPreprocess the data and use labelencoder to sentiment. Tokenize the text data and convert to lowercase. Remove the punctuations and stopwords from the text data. Finally build a vocabulary of all the tokens and assign an index to the vocabulary. Display the index of the word \"happy\".\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n\n", "code": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['review'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nDefine the maximum length of the sequence to 100, and digitize the text to facilitate subsequent text processing.\nDisplay the numericalized representation of the first text.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['review'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n", "code": "# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 100  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nSplit the dataset into training and testing sets using 0.2 as the test size, then define the train_loader and test_loader. Set batch size as 64.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['review'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 100  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\n", "code": "import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "\n\nDefine a \"SentimentAnalysisLSTM\" model using vocab_size，embedding_dim, hidden_dim, output_dim, n_layers. Put the model to device.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['review'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 100  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n", "code": "import torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nDefine the BCELoss loss function and the Adam optimizer. Train the model for 3 epochs. Display the loss for the last epoch(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['review'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 100  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\n", "code": "import torch.optim as optim\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 2:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nEvaluate the model performance on the test set. Report the model's accuracy(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset02.csv', encoding='latin-1')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['review'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 100  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\nimport torch.optim as optim\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 2:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n\n", "code": "import numpy as np\n\nmodel.eval()\npredictions, truths = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = model(inputs)\n        predictions.append(outputs.squeeze())\n        truths.append(labels)\n        \npredictions = torch.cat(predictions).cpu().numpy()\ntruths = torch.cat(truths).cpu().numpy()\npredictions = np.round(predictions)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(truths, predictions)\nround(accuracy, 2)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "File Path: 'data/pytorch_dataset01.csv'\n\nLoad the dataset from the file path into a pandas DataFrame. Display the first 5 rows.\n", "pre_code": "", "code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n", "library": ["pandas"], "exlib": ["pandas"]}
{"prompt": "\n\nPreprocess the data and use labelencoder to sentiment. Tokenize the text data and convert to lowercase. Remove the punctuations and stopwords from the text data. Finally build a vocabulary of all the tokens and assign an index to the vocabulary. Display the index of the word \"happy\".\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n\n", "code": "import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n", "library": ["nltk"], "exlib": ["nltk"]}
{"prompt": "\n\nDefine the maximum length of the sequence to 20, and digitize the text to facilitate subsequent text processing.\nDisplay the numericalized representation of the first text.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n", "code": "# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 20  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nSplit the dataset into training and testing sets using 0.2 as the test size, then define the train_loader and test_loader. Set batch size as 64.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 20  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\n", "code": "import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
{"prompt": "\n\nDefine a \"SentimentAnalysisLSTM\" model for NLP using vocab_size，embedding_dim, hidden_dim, output_dim, n_layers. Put the model to device.\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 20  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\n", "code": "import torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nDefine the BCELoss loss function and the Adam optimizer. Train the model for 5 epochs. Display the loss for the last epoch(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 20  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\n", "code": "import torch.optim as optim\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 4:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n", "library": ["pytorch"], "exlib": ["pytorch"]}
{"prompt": "\n\nEvaluate the model performance on the test set. Report the model's accuracy(Keep to two decimal places).\n\n", "pre_code": "import pandas as pd\n\ntweets_df = pd.read_csv('data/pytorch_dataset01.csv')\ntweets_df.head()\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\n\nnltk.download('punkt')\nnltk.download('stopwords')\ntweets_df['sentiment'] = LabelEncoder().fit_transform(tweets_df['sentiment'])\n\ndef preprocess_and_tokenize(texts):\n    vocabulary = Counter()\n    processed_texts = []\n    stop_words = set(stopwords.words('english'))\n    \n    for text in texts:\n        # 分词\n        words = word_tokenize(text.lower())\n        \n        # 去除停用词和非字母字符\n        words = [word for word in words if word.isalpha() and word not in stop_words]\n        \n        # 更新词汇表\n        vocabulary.update(words)\n        \n        # 添加处理后的文本到列表\n        processed_texts.append(words)\n    \n    return processed_texts, vocabulary\n\n# 应用预处理和分词\nprocessed_texts, vocab = preprocess_and_tokenize(tweets_df['text'])\n\n# 为词汇分配索引\nword_to_index = {word: i+2 for i, word in enumerate(vocab)}  # 索引从 2 开始，留出 0 和 1\nword_to_index['&lt;pad&gt;'] = 0  # 填充标记\nword_to_index['&lt;unk&gt;'] = 1  # 未知词标记\n\n# 展示“happy”的数值标签\nword_to_index['happy']\n\n# 文本数值化\ndef numericalize(tokenized_texts, word_to_index, max_length):\n    numericalized_texts = []\n    for tokens in tokenized_texts:\n        numericalized_text = [word_to_index.get(word, word_to_index['&lt;unk&gt;']) for word in tokens]\n        # 填充或截断\n        numericalized_text = numericalized_text[:max_length] + [word_to_index['&lt;pad&gt;']] * (max_length - len(numericalized_text))\n        numericalized_texts.append(numericalized_text)\n    return numericalized_texts\n\nmax_length = 20  # 定义序列最大长度\nnumericalized_texts = numericalize(processed_texts, word_to_index, max_length)\n# 展示第一条文本的数值化表示\nnumericalized_texts[0]\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\n\nclass TextDataset(Dataset):\n    def __init__(self, texts, labels):\n        self.texts = texts\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.float)\n\n# 创建数据集\ndataset = TextDataset(numericalized_texts, tweets_df['sentiment'])\n\ntrain_data, test_data = train_test_split(dataset, test_size=0.2, random_state=42)\n\n# 创建 DataLoader\nbatch_size = 64\ntrain_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\ntest_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n\nimport torch.nn as nn\n\nclass SentimentAnalysisLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.5):\n        super(SentimentAnalysisLSTM, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        embedded = self.embedding(x)\n        lstm_out, hidden = self.lstm(embedded)\n        out = self.dropout(lstm_out)\n        out = self.fc(out[:, -1])\n        return self.sigmoid(out)\n\nvocab_size = len(word_to_index)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = SentimentAnalysisLSTM(vocab_size, embedding_dim=400, hidden_dim=256, output_dim=1, n_layers=2)\nmodel.to(device)\n\nimport torch.optim as optim\n\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nepochs = 5\nfor epoch in range(epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        loss = criterion(outputs.squeeze(), labels.float())\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    \n    if epoch == 4:\n        final_loss = round(running_loss/len(train_loader), 2)\n        print(final_loss)\n\n", "code": "import numpy as np\n\nmodel.eval()\npredictions, truths = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        \n        outputs = model(inputs)\n        predictions.append(outputs.squeeze())\n        truths.append(labels)\n        \npredictions = torch.cat(predictions).cpu().numpy()\ntruths = torch.cat(truths).cpu().numpy()\npredictions = np.round(predictions)\n\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(truths, predictions)\nround(accuracy, 2)\n", "library": ["pytorch", "sklearn"], "exlib": ["pytorch", "sklearn"]}
